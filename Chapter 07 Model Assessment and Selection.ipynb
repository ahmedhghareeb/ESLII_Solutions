{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.4\n",
    "\n",
    "By definition, the expectation of optimism over training sets is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}-\\overline{\\mbox{err}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(Y_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}-\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}\\right]\\right)\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(\\mathbf{Y}_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}-\\mathbb{E}_{\\mathbf{y}}\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that $\\mathbf{Y}^0$ is another realization of $f(x)+\\varepsilon$, so it is independent of $\\mathbf{y}$, and thus $\\hat{f}(x)$, which is a linear function of $\\mathbf{y}$.\n",
    "\n",
    "As a result, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(\\mathbf{Y}_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2} & =\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}-2\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)+\\hat{f}\\left(x_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}\\right]-2\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{f}\\left(x_{i}\\right)^{2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "At the same time:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2} & =\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}-2y_{i}\\hat{f}\\left(x_{i}\\right)+\\hat{f}\\left(x_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}\\right]-2\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{f}\\left(x_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{f}\\left(x_{i}\\right)^{2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "We also know that $y_i$ and $\\mathbf{Y}^0_i$ are iid, so $$\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}\\right]=\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}\\right].$$\n",
    "\n",
    "With all the above relations, and that $\\hat{y}_i=\\hat{f}(x_i)$, we can conclude that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}-\\overline{\\mbox{err}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{f}\\left(x_{i}\\right)-\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)\\right]\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{y_{i}}-\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\right]\\hat{f}\\left(x_{i}\\right)\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{y_{i}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\right]\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{y}_{i}\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\mbox{Cov}\\left(y_{i},\\hat{y}_{i}\\right).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.5\n",
    "Let's assume that the true model is: $\\mathbf{y}=f(\\mathbf{x)}+\\mathbf{\\varepsilon}$, \n",
    "where $\\varepsilon$ is a $N$ vector of iid random variables with mean $0$ and variance $\\sigma_{\\varepsilon}$.\n",
    "\n",
    "With some matrix algebra we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{y}_{i},y_{i}\\right) & =\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)_{ii}\\\\\n",
    " & =\\mbox{Tr}\\left[\\mbox{Cov}\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left[\\mbox{Cov}\\left(\\mathbf{Sy},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left[\\mathbf{S}\\mbox{Cov}\\left(\\mathbf{y},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left(\\mathbf{S}\\sigma_{\\varepsilon}^{2}\\mathbf{I}\\right)\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\mathbf{I}\\right)\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "It is worth mentioning that, in simple linear regression,\n",
    "$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}$, and in ridge regression with parameter $\\lambda$, \n",
    "$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}.$\n",
    "\n",
    "We can easily prove that the trace for the former is $p$ and for the latter it is:\n",
    "$\\sum_{i=1}^{p}\\frac{d_{i}^{2}}{d_{i}^{2}+\\lambda}$, where $d_i$ is the $i$th singular value of the $N\\times p$ matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.7\n",
    "Using the approximation $1/(1-x)^2 \\approx 1+2x$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{GCV} & =\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\frac{y_{i}-\\hat{f}\\left(x_{i}\\right)}{1-\\mbox{Tr}\\left(\\mathbf{S}\\right)/N}\\right]^{2}\\\\\n",
    " & \\approx\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2}\\left(1+\\frac{2\\mbox{Tr}\\left(\\mathbf{S}\\right)}{N}\\right)\\\\\n",
    " & =\\overline{\\mbox{err}}+2\\frac{\\mbox{Tr}\\left(\\mathbf{S}\\right)}{N}\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2}.\n",
    "\\end{align*}\n",
    "\n",
    "The trace of $\\mathbf{S}$ is the effective degree of free down of the linear smoother $\\mathbf{S}$. If we use average square error of estimator $\\hat{f}$ as estimator of the true error:\n",
    "\n",
    "$$\\hat{\\sigma}_{\\varepsilon}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2},$$ then we know that GCV is an approximation of $C_p$ when $\\mbox{Tr}\\left(\\mathbf{S}\\right)/N$ is small. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
