{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.4\n",
    "\n",
    "By definition, the expectation of optimism over training sets is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}-\\overline{\\mbox{err}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(Y_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}-\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}\\right]\\right)\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(\\mathbf{Y}_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}-\\mathbb{E}_{\\mathbf{y}}\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that $\\mathbf{Y}^0$ is another realization of $f(x)+\\varepsilon$, so it is independent of $\\mathbf{y}$, and thus $\\hat{f}(x)$, which is a linear function of $\\mathbf{y}$.\n",
    "\n",
    "As a result, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(\\mathbf{Y}_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2} & =\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}-2\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)+\\hat{f}\\left(x_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}\\right]-2\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{f}\\left(x_{i}\\right)^{2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "At the same time:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2} & =\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}-2y_{i}\\hat{f}\\left(x_{i}\\right)+\\hat{f}\\left(x_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}\\right]-2\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{f}\\left(x_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{f}\\left(x_{i}\\right)^{2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "We also know that $y_i$ and $\\mathbf{Y}^0_i$ are iid, so $$\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}\\right]=\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}\\right].$$\n",
    "\n",
    "With all the above relations, and that $\\hat{y}_i=\\hat{f}(x_i)$, we can conclude that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}-\\overline{\\mbox{err}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{f}\\left(x_{i}\\right)-\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)\\right]\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{y_{i}}-\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\right]\\hat{f}\\left(x_{i}\\right)\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{y_{i}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\right]\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{y}_{i}\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\mbox{Cov}\\left(y_{i},\\hat{y}_{i}\\right).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.5\n",
    "Let's assume that the true model is: $\\mathbf{y}=f(\\mathbf{x)}+\\mathbf{\\varepsilon}$, \n",
    "where $\\varepsilon$ is a $N$ vector of iid random variables with mean $0$ and variance $\\sigma_{\\varepsilon}$.\n",
    "\n",
    "With some matrix algebra we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{y}_{i},y_{i}\\right) & =\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)_{ii}\\\\\n",
    " & =\\mbox{Tr}\\left[\\mbox{Cov}\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left[\\mbox{Cov}\\left(\\mathbf{Sy},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left[\\mathbf{S}\\mbox{Cov}\\left(\\mathbf{y},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left(\\mathbf{S}\\sigma_{\\varepsilon}^{2}\\mathbf{I}\\right)\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\mathbf{I}\\right)\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "It is worth mentioning that, in simple linear regression,\n",
    "$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}$, and in ridge regression with parameter $\\lambda$, \n",
    "$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}.$\n",
    "\n",
    "We can easily prove that the trace for the former is $p$ and for the latter it is:\n",
    "$\\sum_{i=1}^{p}\\frac{d_{i}^{2}}{d_{i}^{2}+\\lambda}$, where $d_i$ is the $i$th singular value of the $N\\times p$ matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.7\n",
    "Using the approximation $1/(1-x)^2 \\approx 1+2x$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{GCV} & =\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\frac{y_{i}-\\hat{f}\\left(x_{i}\\right)}{1-\\mbox{Tr}\\left(\\mathbf{S}\\right)/N}\\right]^{2}\\\\\n",
    " & \\approx\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2}\\left(1+\\frac{2\\mbox{Tr}\\left(\\mathbf{S}\\right)}{N}\\right)\\\\\n",
    " & =\\overline{\\mbox{err}}+2\\frac{\\mbox{Tr}\\left(\\mathbf{S}\\right)}{N}\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2}.\n",
    "\\end{align*}\n",
    "\n",
    "The trace of $\\mathbf{S}$ is the effective degree of free down of the linear smoother $\\mathbf{S}$. If we use average square error of estimator $\\hat{f}$ as estimator of the true error:\n",
    "\n",
    "$$\\hat{\\sigma}_{\\varepsilon}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2},$$ then we know that GCV is an approximation of $C_p$ when $\\mbox{Tr}\\left(\\mathbf{S}\\right)/N$ is small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.8\n",
    "\n",
    "The idea is to find a way to construct $\\alpha$ given the classes of $z^1,\\dots,z^l$, \n",
    "such that $\\alpha z^i$ is in $(0, \\pi)$ if $z^i$ is of class 0 and in $(\\pi, 2\\pi)$ is \n",
    "$z^i$ is of class 1.\n",
    "\n",
    "We want to achieve the goal that the value of $\\sin\\left(\\alpha z^{i}\\right)$ is dominantly determined by $y^i$. \n",
    "Let's denote the class of $z^i$ by $y_i$, then $\\alpha$ must be a function of $y_1,\\dots, y_l$.\n",
    "Let's consider the case that $\\alpha$ is a linear combition of $y_i$:\n",
    "\n",
    "$$\\alpha = \\sum_i^la_iy_i.$$\n",
    "\n",
    "We want the value of $\\sin\\left(\\alpha z^{i}\\right)$ to be insensitive to the values of $y^j$, where $j \\ne i$.\n",
    "The are only two ways to for the sine function to be insensitive to the change of its independnent variable: either the change is too small compared with $\\pi$, or the change is a multiple of $2\\pi$.\n",
    "Let's futher look at the decomposition of the independent variable:\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha z^{j} & =\\sum_{i=1}^{l}a_{i}y^{i}z^{j}\\\\\n",
    " & =\\sum_{i=1}^{j-1}a_{i}y^{i}z^{j}+a_{j}y^{j}z^{j}+\\sum_{i=j+1}^{l}a_{i}y^{i}z^{j}\n",
    "\\end{align*}\n",
    "\n",
    "In order to achieve the goal, we want the value of $a_{j}y^{j}z^{j}$ to be close to $\\pi y^j$, the value of $\\sum_{i=1}^{j-1}a_{i}y^{i}z^{j}$ to be close to 0, and the value of $\\sum_{i=j+1}^{l}a_{i}y^{i}z^{j}$ to be a multiple of $2\\pi$.\n",
    "\n",
    "An obvious choice is $a_i=10^i\\pi$, in that case:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{j-1}a_{i}y^{i}z^{j} & =\\pi z^{j}\\sum_{i=1}^{j-1}10^{i}y^{i}\\\\\n",
    " & \\in\\left[0,\\pi\\sum_{i=1}^{j-1}10^{i-j}\\right)\\\\\n",
    " & =\\left[0,\\pi\\frac{1-10^{1-j}}{9}\\right)\\\\\n",
    " & \\in\\left[0,\\pi\\right)\n",
    "\\end{align*}\n",
    "\n",
    "$$a_{j}y^{j}z^{j}=\\pi y^{j}=\\begin{cases}\n",
    "0 & y^{j}=0\\\\\n",
    "\\pi & y^{j}=1\n",
    "\\end{cases}$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=j+1}^{l}a_{i}y^{i}z^{j} & =\\pi\\sum_{i=j+1}^{l}10^{i-j}y^{i}\\\\\n",
    " & =\\pi\\sum_{i=1}^{l-j}10^{i}y^{i}\\\\\n",
    " & =2\\pi\\cdot5\\sum_{i=0}^{l-j-1}10^{i}y^{i}\n",
    "\\end{align*}\n",
    "\n",
    "As a result, we can sepete the points $z^j$ with lable $y^j$ in this way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sin(az^{j})= & \\sin\\left(\\pi\\sum_{i=1}^{j-1}10^{i-j}y^{i}+\\pi y^{j}\\right)\\\\\n",
    "= & \\begin{cases}\n",
    "\\sin\\left[0,\\pi\\right) & y^{j}=0\\\\\n",
    "\\sin\\left[\\pi,2\\pi\\right) & y^{j}=1\n",
    "\\end{cases}\\\\\n",
    "\\in & \\begin{cases}\n",
    "\\left[0,1\\right) & y^{j}=0\\\\\n",
    "\\left(-1,0\\right] & y^{j}=1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "This is not satisfying enough, because sometimes the function value can be $0$ and thus lies in the class boundary.\n",
    "To solve this problem we introduce an additional term to $a$,\n",
    "$$a=\\pi\\left(1+\\sum_{i=1}^{l}10^{i}y^{i}\\right)$$\n",
    "\n",
    "This additional small term will perturb the function slightly away from 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(\\pi+\\sum_{i=1}^{j-1}a_{i}y^{i}\\right)z^{j} & =\\pi z^{j}\\left(1+\\sum_{i=1}^{j-1}10^{i}y^{i}\\right)\\\\\n",
    " & \\in\\left[\\pi10^{-j},\\pi\\sum_{i=0}^{j-1}10^{i-j}\\right)\\\\\n",
    " & =\\left[\\pi10^{-j},\\pi\\frac{1-10^{-j}}{9}\\right)\\\\\n",
    " & \\subset\\left(0,\\pi\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Finally we proved that there exists $a=\\pi\\left(1+\\sum_{i=1}^{l}10^{i}y^{i}\\right)$, \n",
    "such that the function $\\sin(ax)$ can shatter the points $\\{10^{-j}\\}_{j=1}^l$ \n",
    "with arbitrarily large $l$. That is to say, the VC dimmension of the set of functions $\\{I(\\sin(ax))>0\\}$ is infinite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
