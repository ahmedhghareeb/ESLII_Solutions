{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.2\n",
    "\n",
    "This excercise covers the statistical tests for the parameters and the prediction. We are going to derive the relationship, under the same confidence level, between the interval projected by the confidence parameter set and the confidence interval itself. The conclusion is: \n",
    "\n",
    "**In linear regression model assuming iid normal errors, with the same confidence level, the  interval of expected prediction generated by the confidence paramter set strictly covers the confidence interval of the expected prediction symmetrically.**\n",
    "\n",
    "By expected prediction we mean the true value of the dependent variable less the error term, which is $\\mathrm{E}[y_0]=f(x_0)=x_0^T\\beta$.\n",
    "\n",
    "The proof is done without any assumption on the problem setup. Namely lets denote the true model as:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X} \\beta + \\mathbf{\\varepsilon},$$\n",
    "where $\\mathbf{y}$ and $\\mathbf{X}$ are the training set with $N$ entries. $\\mathbf{X}$ is a $N\\times p$ matrix, and in this perticular problem, the first colume of $\\mathbf{X}$ is all one, the second column is $X_i$, the third column is $X_i^2$, etc. We further define deviation of the fitted parameters from its true value by $\\beta^* = \\hat{\\beta}-\\beta$, then the deviation of the prediction is $\\hat{f}(x_0)-f(x_0)=x_0^T\\beta^*$. We further use this operator: $(\\cdot)^{(1-\\alpha)}$ to denote the $1-\\alpha$ quantile of a random variable.\n",
    "We know $x_0^T\\beta^*$ is normally distributed with $0$ mean, and the confidence interval of $f(x_0)$ is simply:\n",
    "$$\\left(\\hat{f}(x_0)-\\left(x_0^T\\beta^*\\right)^{(1-\\alpha/2)},\\,\n",
    "\\hat{f}(x_0)+\\left(x_0^T\\beta^*\\right)^{(1-\\alpha/2)}\\right).$$\n",
    "\n",
    "The prediction interval generated by confidence parameter set is slighly more complicated. One may already noticed that there is a typo in 3.15, where the $\\hat{\\sigma}^2$ should be replaced by $\\sigma^2$. The $1-\\alpha$ confidence set of $\\beta$ is:\n",
    "\n",
    "$$C_{\\beta}=\\left\\{ \\beta\\vert\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*}\\le\\left(\\sigma^{2}\\chi_{p}^{2}\\right)^{(1-\\alpha)}=\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\right\\},$$\n",
    "and the interval of expected prediction generated by this confidence set is:\n",
    "$$\\left(\\hat{f}(x_{0})+\\min_{\\beta^{*}\\in C_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right),\\,\\hat{f}(x_{0})+\\max_{\\beta^{*}\\in C_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)\\right).$$\n",
    "\n",
    "The two end points of the above interval are determined by optimization on the linear objective function $x_{0}^{T}\\beta^{*}$ with quadratic constraint $C_\\beta$. Since the objective function is linear, we know it obtains both its mininum and maximum at the boundary of the contraint. The constraint set is symmetric about the origin, we further know that the minimum and maximum are symmetric about the origin as well. Namely, let's denote the boundary of $C_\\beta$ as:\n",
    "$$B_{\\beta}=\\left\\{ \\beta\\vert\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*}=\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\right\\}.$$\n",
    "\n",
    "We simplify the second interval as:\n",
    "$$\\left(\\hat{f}(x_{0})-\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right),\\,\\hat{f}(x_{0})+\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)\\right).$$\n",
    "\n",
    "The plane $x_{0}^{T}\\beta^{*}=\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)$ must be tangential to $B_\\beta$, we can use the following two equations to solve for\n",
    "$\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\\\\n",
    "\\frac12\\nabla_{\\beta^{*}}\\left(\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*}\\right)=\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\lambda x_{0}=\\lambda\\nabla_{\\beta^{*}}\\left(x_{0}^{T}\\beta^{*}\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "The solution is obtained by eliminating $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\lambda x_{0}\\\\\n",
    "\\Rightarrow\\beta^{*} & =\\lambda\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    "\\Rightarrow\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\lambda^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\lambda^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\\\\n",
    "\\Rightarrow x_{0}^{T}\\beta^{*} & =\\lambda x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\sqrt{\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On the other hand, we know that \n",
    "$$x_{0}^{T}\\beta^{*}=x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\varepsilon,$$ \n",
    "and that \n",
    "$$\n",
    "\\mathrm{Var}\\left(x_{0}^{T}\\beta^{*}\\right)=x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathrm{Cov}(\\varepsilon)\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}=\\sigma^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}.\n",
    "$$\n",
    "\n",
    "The distribution of $x_{0}^{T}\\beta^{*}$ is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{0}^{T}\\beta^{*} & \\sim\\mathcal{N}\\left(0,\\,\\sigma^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\right)\n",
    "\\end{align*}.\n",
    "$$\n",
    "$\\sigma^2$ is unkown, so the confidence interval of $x_{0}^{T}\\beta^{*}$ is based on $t$ distribution:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{x_{0}^{T}\\beta^{*}}{\\sqrt{\\hat{\\sigma}^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}} & \\sim t_{N-p}\\\\\n",
    "\\left(x_{0}^{T}\\beta^{*}\\right)^{(1-\\alpha/2)} & =t_{N-p}^{(1-\\alpha/2)}\\sqrt{\\hat{\\sigma}^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\\\\\n",
    " & =\\sqrt{\\hat{\\sigma}^{2}\\left(t_{N-p}^{(1-\\alpha/2)}\\right)^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\\\\\n",
    " & =\\sqrt{\\hat{\\sigma}^{2}F_{1,N-p}^{(1-\\alpha)}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since\n",
    "$$\n",
    "\\begin{align*}\n",
    "pF_{p,N-p}^{(1-\\alpha)} & =\\left(\\frac{\\chi_{p}^{2}}{\\chi_{N-p}^{2}/(N-p)}\\right)^{(1-\\alpha)}\\\\\n",
    " & >\\left(\\frac{\\chi_{1}^{2}}{\\chi_{N-p}^{2}/(N-p)}\\right)^{(1-\\alpha)}\\\\\n",
    " & =F_{1,N-p}^{(1-\\alpha)}\n",
    "\\end{align*}\n",
    "$$\n",
    "We know:\n",
    "$$\n",
    "\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)>\\left(x_{0}^{T}\\beta^{*}\\right)^{(1-\\alpha/2)}.\n",
    "$$\n",
    "This completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.6\n",
    "\n",
    "With Bayes' rule, we can find the relation that the $\\beta$ maximizing the posterior density is the ridge regression of $\\beta$ with a parameter of $\\lambda = \\frac{\\sigma^2}{\\tau^2}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\beta|\\mathbf{y}) & =\\frac{p(\\mathbf{y}|\\beta)}{p(\\mathbf{y})}\\cdot p(\\beta)\\\\\n",
    " & =\\frac{1}{p(\\mathbf{y})}\\cdot p(\\mathbf{y}|\\beta)\\prod_{i=1}^{p}\\frac{1}{\\sqrt{2\\pi\\tau^{2}}}e^{-\\frac{\\beta_{i}^{2}}{2\\tau^{2}}}\\\\\n",
    " & =\\frac{1}{p(\\mathbf{y})}\\cdot\\prod_{j=1}^{p}\\frac{1}{\\sqrt{2\\pi\\tau^{2}}}e^{-\\frac{\\beta_{j}^{2}}{2\\tau^{2}}}\\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-\\mathbf{X}\\beta)^{2}}{2\\sigma^{2}}}\\\\\n",
    "\\Rightarrow-\\ln p(\\beta|\\mathbf{y}) & =\\ln p(\\mathbf{y})+\\frac{p}{2}\\ln\\left(2\\pi\\tau^{2}\\right)+\\frac{N}{2}\\ln\\left(2\\pi\\sigma^{2}\\right)+\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\frac{1}{2\\tau^{2}}\\sum_{j=1}^{p}\\beta_{j}^{2}\\\\\n",
    " & =\\ln p(\\mathbf{y})+\\frac{p}{2}\\ln\\left(2\\pi\\tau^{2}\\right)+\\frac{N}{2}\\ln\\left(2\\pi\\sigma^{2}\\right)+\\frac{1}{2\\sigma^{2}}\\left(\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\frac{\\sigma^{2}}{\\tau^{2}}\\sum_{j=1}^{p}\\beta_{j}^{2}\\right)\\\\\n",
    "\\Rightarrow\\mathrm{argmax}_{\\beta}p(\\beta|\\mathbf{y}) & =\\mathrm{argmin}_{\\beta}\\left(\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\frac{\\sigma^{2}}{\\tau^{2}}\\sum_{j=1}^{p}\\beta_{j}^{2}\\right)\\\\\n",
    "\\mbox{together with }\\hat{\\beta}(\\lambda)^{\\mathrm{ridge}} & =\\mathrm{argmin}_{\\beta}\\left(\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\lambda\\sum_{j=1}^{p}\\beta_{j}^{2}\\right)\\\\\n",
    "\\Rightarrow\\mathrm{argmax}_{\\beta}p(\\beta|\\mathbf{y}) & =\\hat{\\beta}(\\frac{\\sigma^{2}}{\\tau^{2}})^{\\mathrm{ridge}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We know that $p(\\beta|\\mathbf{y})$ is Gaussian, therefore, the density reaches its maximum (mode) at its mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.12\n",
    "\n",
    "In the augmented regression, the true model is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left[\\begin{array}{c}\n",
    "\\mathbf{y}\\\\\n",
    "0\n",
    "\\end{array}\\right]= & \\left[\\begin{array}{c}\n",
    "\\mathbf{X}\\\\\n",
    "\\sqrt{\\lambda}\\mathbf{I}\n",
    "\\end{array}\\right]\\beta+\\mathbf{\\varepsilon}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This will produce the least square estimator of $\\beta$ as the \n",
    "following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} & =\\left(\\left[\\begin{array}{cc}\n",
    "\\mathbf{X}^{T} & \\sqrt{\\lambda}\\mathbf{I}\\end{array}\\right]\\cdot\\left[\\begin{array}{c}\n",
    "\\mathbf{X}\\\\\n",
    "\\sqrt{\\lambda}\\mathbf{I}\n",
    "\\end{array}\\right]\\right)^{-1}\\cdot\\left[\\begin{array}{cc}\n",
    "\\mathbf{X}^{T} & \\sqrt{\\lambda}\\mathbf{I}\\end{array}\\right]\\cdot\\left[\\begin{array}{c}\n",
    "\\mathbf{y}\\\\\n",
    "0\n",
    "\\end{array}\\right]\\\\\n",
    " & =\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\cdot\\left(\\mathbf{X}^{T}\\mathbf{y}+\\sqrt{\\lambda}\\mathbf{I}\\right)\\\\\n",
    " & =\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is exactly the same as the formula in ridge regression (3.44)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.23\n",
    "## (a)\n",
    "$\\mathbf{x}_j$ is the $j$th column of $\\mathbf{X}$, also the realizations of the $j$th regressor. Let's define $\\mathbf{e}_j$ as the column vector with the $j$th entry being $1$ and all the rest $p-1$ entries $0$. In this way we can denote $\\mathbf{x}_j$ as $\\mathbf{Xe}_j$.\n",
    "\n",
    "The covariances of each $\\mathbf{x}_j$ with the residuals can be conveniently written as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right\\rangle \\right| & =\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}-\\alpha\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\right\\rangle \\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\left(\\mathbf{I}-\\alpha\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\right)\\mathbf{y}\\right\\rangle \\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\mathbf{x}_{j}^{T}\\left(\\mathbf{I}-\\alpha\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\right)\\mathbf{y}\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha(\\mathbf{Xe}_{j})^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha\\mathbf{e}_{j}^{T}\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha\\mathbf{e}_{j}^{T}\\mathbf{X}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha\\mathbf{x}_{j}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =(1-\\alpha)\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =(1-\\alpha)\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}\\right\\rangle \\right|\\\\\n",
    " & =(1-\\alpha)\\lambda.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## (b)\n",
    "We know that the response $\\mathbf{y}$ and each $\\mathbf{x}_j$ has been normalized to have zero mean. So it is obvious that $\\mathbf{u}(\\alpha)$ also has zero mean, because it is a linear combination of zero-mean vectors. Let's futher denote $\\mathbf{P}$ as the matrix: $\\mathbf{P}=\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T$, and it is easy to varify that it is idempotent: $\\mathbf{PP}=\\mathbf{P}$. Then we know that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Var}\\left(\\mathbf{y}-\\mathbf{u}(\\alpha)\\right) & =\\mathrm{Var}\\left(\\mathbf{y}-\\alpha\\mathbf{Py}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\mathbf{y}^{T}\\left(\\mathbf{I}-\\alpha\\mathbf{P}\\right)\\left(\\mathbf{I}-\\alpha\\mathbf{P}\\right)\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\mathbf{y}^{T}\\left(\\mathbf{I}-2\\alpha\\mathbf{P}+\\alpha^{2}\\mathbf{P}\\right)\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\mathbf{y}^{T}\\left(\\alpha\\left(2-\\alpha\\right)\\mathbf{I}-\\alpha\\left(2-\\alpha\\right)\\mathbf{P}+(1-\\alpha)^{2}\\mathbf{I}\\right)\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\alpha\\left(2-\\alpha\\right)\\mathbf{y}^{T}\\left(\\mathbf{I}-\\mathbf{P}\\right)\\mathbf{y}+(1-\\alpha)^{2}\\mathbf{y}^{T}\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\alpha\\left(2-\\alpha\\right)\\mathrm{RSS}+(1-\\alpha)^{2}\\frac{\\mathbf{y}^{T}\\mathbf{y}}{N}\\\\\n",
    " & =(1-\\alpha)^{2}+\\frac{\\alpha\\left(2-\\alpha\\right)}{N}\\cdot\\mathrm{RSS}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally, we proved that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cor}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right) & =\\frac{\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}{\\sqrt{\\mathrm{Var}\\left(\\mathbf{x}_{j}\\right)}\\cdot\\sqrt{\\mathrm{Var}\\left(\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}}\\\\\n",
    " & =\\frac{(1-\\alpha)\\lambda}{1\\cdot\\sqrt{\\alpha\\left(2-\\alpha\\right)\\frac{\\mathrm{RSS}}{N}+(1-\\alpha)^{2}}}\\\\\n",
    " & =\\frac{(1-\\alpha)}{\\sqrt{(1-\\alpha)^{2}+\\frac{\\alpha\\left(2-\\alpha\\right)}{N}\\cdot\\mathrm{RSS}}}\\cdot\\lambda.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## (c)\n",
    "\n",
    "We know that the corelations between the $p$ regressors $\\mathbf{x}_j$ and the residul as a function of $\\alpha$ are the same, which is formlated in (b). Let's reformulate it as:\n",
    "$$\\frac{\\lambda}{\\sqrt{(1-\\frac{\\mathrm{RSS}}{N})+\\frac{1}{(1-\\alpha)^{2}}\\frac{\\mathrm{RSS}}{N}}}$$\n",
    "\n",
    "It is obvious that it is monotonically decreasing as $\\alpha$ goes from $0$ to $1$.\n",
    "\n",
    "In the LAR algorithm, as the corelations between the residual and regressors in the active set monotonically decreases, at some point, this corelation will become equal to the largest corelation between the residual and remaining inactive regressors. When this happens, we move this regressor from inactive set to active set, initialize its coefficient as $0$, and recompute the moving direction with the updated active regressors. As we can see, during this whole process, the corelations between the residual and active regressors are kept same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.24\n",
    "In Ex. 3.23, we just proved that, in LAR algorithm, the residual $\\mathbf{y}-\\mathbf{u}(\\alpha)$ has the same corelation with all the current predictor vectors $\\mathbf{x}_j$. In geometry, this corelation is just the cosine of the angle between $\\mathbf{y}-\\mathbf{u}(\\alpha)$ and $\\mathbf{x}_j$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cor}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right) & =\\frac{\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}{\\sqrt{\\mathrm{Var}\\left(\\mathbf{x}_{j}\\right)}\\cdot\\sqrt{\\mathrm{Var}\\left(\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}}\\\\\n",
    " & =\\frac{\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right\\rangle }{\\sqrt{\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{x}_{j}\\right\\rangle }\\cdot\\sqrt{\\left\\langle \\mathbf{y}-\\mathbf{u}(\\alpha),\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right\\rangle }}\\\\\n",
    " & =\\cos\\left(\\mathrm{angle}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In this problem, we try to prove that the new direction has the same angle to all the active regressors. We know that the variances of $\\mathbf{x}_j$ are all $1$ hence independent of $j$. All we need to do is to prove that the covariances between $\\mathbf{x}_j$ and $\\mathbf{u}_k$ are independent of $j$.\n",
    "\n",
    "At the $k$th step, this covariance is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{u}_{k}\\right) & =\\frac{1}{N}\\left(\\mathbf{X}_{\\mathcal{A}_{k}}\\mathbf{e}_{j}\\right)^{T}\\mathbf{u}_{k}\\\\\n",
    " & =\\frac{1}{N}\\mathbf{e}_{j}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}\\left(\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}\\right)^{-1}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{r}_{k}\\\\\n",
    " & =\\frac{1}{N}\\mathbf{e}_{j}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{r}_{k}\\\\\n",
    " & =\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{r}_{k}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "$\\mathbf{r}_k$ is the residual of at the end of the $k-1$th step, we know from Ex. 3.23 that its corelations with all the $k-1$ regressors in the previous step are the same. As for the newly added regressor, it is also the same because that's how it is picked. We just proved that, at the begining of a step, the covariances between the moving direction of the predictor $\\hat{\\mathbf{y}}$ and the active regressors and the covariances between the residual and the active regressors are the same. Geometrically speaking, the moving direction has the same angle with all the active regressors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
