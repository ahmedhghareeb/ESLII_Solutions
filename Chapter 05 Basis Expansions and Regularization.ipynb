{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 5.15\n",
    "\n",
    "This exercise is about reproducing kernel Hilbert space, which, I believe is the most important topic in this chapter. \n",
    "For many classification problems, with a properly chosen kernel, the optimization formula will be significantly simplified. \n",
    "\n",
    "Let's first review how reproducing kernel are defined.\n",
    "A Hilbert space is reproducing kernel Hilbert space when reproducing kernels can be defined. \n",
    "For simplicity, let's consider the Hilbert space $\\mathcal{H} : \\mathbb{R}^p \\rightarrow \\mathbb{R}$.\n",
    "With Reisz representation theorem on Hilbert space $\\mathcal{H}$, we know that for any bounded linear functional $\\phi$ in its dual $\\mathcal{H}^* : (\\mathbb{R}^p \\rightarrow \\mathbb{R})\\rightarrow \\mathbb{R}$, there is an unique vector $u \\in \\mathcal{H}$ that $\\phi(f) = \\langle f,u\\rangle $.\n",
    "We denote such vector $u$ as the representative of $\\phi$ in $\\mathcal{H}$.\n",
    "\n",
    "Now we are interested in the one particular type of linear functional: the Dirac evaluation functional. \n",
    "This functional $\\delta_x$ will map any vector $f \\in \\mathcal{H}$ to its value at the given point $x\\in \\mathbb{R}^p$, that is, $f(x)$.\n",
    "\n",
    "When any Dirac evaluation functional $\\delta_x$ on $\\mathcal{H}$ is bounded, we can have unique representative in $\\mathcal{H}$ for each $\\delta_x$, namely $k_x$.\n",
    "In this case the reproducing kernel is defined as the function $K(x,y)=\\langle k_x, k_y\\rangle _\\mathcal{H}: \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$. \n",
    "When reproducing kernel can be defined on a Hilbert space, we call it reproducing kernel Hilbert space. \n",
    "As we just shown, a Hilbert space is reproducing kernel Hilbert space when all the Dirac evaluation functional on it is bounded. \n",
    "\n",
    "If a function $K(x,y): \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ is symmetric and positive definite:\n",
    "$$\\sum_{i=1}^n\\sum_{j=1}^n c_i c_j K(x_i, x_j) \\ge 0, \\quad \\forall n \\in \\mathbb{N}, c_i, c_j \\in \\mathbb{R}, \n",
    "x_i, x_j \\in \\mathbb{R}^p,$$\n",
    "\n",
    "$K(x,y)$ uniquely defines a RKHS as the completion of linear space spanned by $\\{K(\\cdot, y)| y \\in \\mathbb{R}^p\\}$.\n",
    "$K(x,y)$ also uniquely defines a self-adjoint operator $\\mathcal{H}_K\\rightarrow \\mathcal{H}_K$:\n",
    "\n",
    "$$T_{K}(f)=\\left\\langle K(x,),f\\right\\rangle _{L_{2}}=\\int_{\\mathbb{R}^{p}}K(x,y)f(y)\\mu(\\mathrm{d}y),$$\n",
    "\n",
    "where $\\mu$ is the Lebesgue measure in $\\mathbb{R}^p$, and $\\langle,\\rangle_{L_2}$ is the inner product in $\\mathcal{H}$ that will induce the $L_2$ norm.\n",
    "\n",
    "By spectral theorem, there exists positive decreasing sequence of eigenvalues $\\{\\gamma_i > \\gamma_{i+1} > 0|\\lim_{i\\rightarrow \\infty} \\gamma_i = 0\\}$, \n",
    "and corresponding eigen vectors $\\phi_i$ such that $K(x,y)$ can be represented by this expansion:\n",
    "\n",
    "$$K(x,y) =\\sum_{i=1}^{\\infty}\\gamma_{i}\\phi_{i}(x)\\phi_{i}(y). $$\n",
    "\n",
    "$\\phi_i$ is a orthonormal basis in $L_2\\left(\\mathbb{R}^p\\right)$:\n",
    "$$\\langle \\phi_i, \\phi_j\\rangle_{L_2}=\\delta_{ij}.$$\n",
    "\n",
    "This helps us to define the inner product in $\\mathcal{H}_K$:\n",
    "\n",
    "$$\\left\\langle f,g\\right\\rangle _{\\mathcal{H}_{K}}=\\sum_{i=1}^{\\infty}\\frac{\\left\\langle f,\\phi_{i}\\right\\rangle _{L_{2}}\\left\\langle g,\\phi_{i}\\right\\rangle _{L_{2}}}{\\gamma_{i}}.$$\n",
    "\n",
    "Suppose $f$ has the following decompostion:\n",
    "$$f=\\sum_{j=1}^{\\infty}c_{j}\\phi_{j},$$\n",
    "then the norm induced by the inner product in $\\mathcal{H}_K$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\Vert f\\right\\Vert _{\\mathcal{H}_{K}}^{2} & =\\left\\langle f,f\\right\\rangle _{\\mathcal{H}_{K}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left\\langle f,\\phi_{i}\\right\\rangle _{L_{2}}\\left\\langle f,\\phi_{i}\\right\\rangle _{L_{2}}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left(\\left\\langle \\sum_{j=1}^{\\infty}c_{j}\\phi_{j},\\phi_{i}\\right\\rangle _{L_{2}}\\right)^{2}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left(\\sum_{j=1}^{\\infty}c_{j}\\left\\langle \\phi_{j},\\phi_{i}\\right\\rangle _{L_{2}}\\right)^{2}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left(\\sum_{j=1}^{\\infty}c_{j}\\delta_{ij}\\right)^{2}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{c_{i}^{2}}{\\gamma_{i}}.\n",
    "\\end{align*}\n",
    "\n",
    "This fills the gap of deriving (5.47) in the text book.\n",
    "## (a)\n",
    "\n",
    "\\begin{align*}\n",
    "K(x,y) & =\\langle k_{x},k_{y}\\rangle_{\\mathcal{H}_{K}}=k_{y}(x)=k_{x}(y)\\\\\n",
    "\\Rightarrow K(\\cdot,y) & =k_{y}\\\\\n",
    "\\Rightarrow\\langle K(\\cdot,x_{i}),f\\rangle_{\\mathcal{H}_{K}} & =\\langle k_{x_{i}},f\\rangle_{\\mathcal{H}_{K}}=f(x_{i})\n",
    "\\end{align*}\n",
    "\n",
    "## (b)\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle K(\\cdot,x_{i}),K(\\cdot,x_{j})\\rangle_{\\mathcal{H}_{K}} & =\\langle k_{x_{i}},k_{x_{j}}\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =K(x_{i},x_{j}).\n",
    "\\end{align*}\n",
    "\n",
    "## (c)\n",
    "Since $K(x, x_i)$ has the eigen-decomposition:\n",
    "$$K(x,x_{i})  =\\sum_{l=1}^{\\infty}\\gamma_{l}\\phi_{l}(x)\\phi_{l}(x_{i}). $$\n",
    "\n",
    "We can find the coordinates of $g(x)$ under this eigen basis:\n",
    "\n",
    "\\begin{align*}\n",
    "g(x) & =\\sum_{i=1}^{N}\\alpha_{i}K(x,x_{i})\\\\\n",
    " & =\\sum_{i=1}^{N}\\alpha_{i}\\sum_{l=1}^{\\infty}\\gamma_{l}\\phi_{l}(x)\\phi_{l}(x_{i})\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\left(\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\right)\\phi_{l}(x)\\\\\n",
    " & =\\sum_{l=1}^{\\infty}c_{l}\\phi_{l}(x), \n",
    "\\end{align*}\n",
    "where \n",
    "$$c_{l}=\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i}).$$\n",
    "\n",
    "By defintion, \n",
    "\n",
    "\\begin{align*}\n",
    "J(g) & =\\sum_{l=1}^{\\infty}\\frac{c_{l}^{2}}{\\gamma_{l}}\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\frac{\\left(\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\right)^{2}}{\\gamma_{l}}\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\gamma_{l}\\left(\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\right)^{2}\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\sum_{j=1}^{N}\\alpha_{j}\\phi_{l}(x_{j})\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}\\left(\\sum_{l=1}^{\\infty}\\gamma_{l}\\phi_{l}(x_{i})\\phi_{l}(x_{j})\\right)\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}K(x_{i},x_{j}).\n",
    "\\end{align*}\n",
    "\n",
    "Alternatively, we can use the result that this norm is induced by the inner product:\n",
    "\n",
    "\\begin{align*}\n",
    "J(g) & =\\langle g,g\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\langle\\sum_{i=1}^{N}\\alpha_{i}K(x,x_{i}),\\sum_{j=1}^{N}\\alpha_{j}K(x,x_{j})\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}\\langle K(x,x_{i}),K(x,x_{j})\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}K(x_{i},x_{j}).\n",
    "\\end{align*}\n",
    "\n",
    "## (d)\n",
    "\n",
    "Since $\\rho(x)$ is orthogonal to each $K(x,x_i)$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle K(\\cdot,x_{i}),\\rho\\rangle_{\\mathcal{H}_{K}} & =\\rho(x_{i})=0 & \\Rightarrow\\\\\n",
    "L\\left(y_{i},\\tilde{g}(x_{i})\\right) & =L\\left(y_{i},g(x_{i})+\\rho(x_{i})\\right)=L\\left(y_{i},g(x_{i})\\right) & \\Rightarrow\\\\\n",
    "\\sum_{i=1}^{N}L\\left(y_{i},\\tilde{g}(x_{i})\\right)+\\lambda J\\left(\\tilde{g}\\right) & =\\sum_{i=1}^{N}L\\left(y_{i},g(x_{i})\\right)+\\lambda J\\left(\\tilde{g}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "As we can see, $\\rho(x)$ lives in the perpendicular complement of subspace spanned by $K(x,x_i)$ so natrually it has no contribution to the loss function where only predictions at training set points $x_i$ are relavent. \n",
    "\n",
    "In order to prove the desired result, what remains to be shown is:\n",
    "$$J\\left(\\tilde{g}\\right)\\ge J\\left(g\\right).$$\n",
    "\n",
    "$\\rho(x)$ is orthogonal to the subspace spanned by $K(x,x_i)$, where $g(x)$ lives, so $\\rho(x)$ is also orthogonal to $g(x)$. With Pythagorean theorem, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "J\\left(\\tilde{g}\\right) & =\\langle\\tilde{g},\\tilde{g}\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\langle g,g\\rangle_{\\mathcal{H}_{K}}+\\langle\\rho,\\rho\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =J\\left(g\\right)+J\\left(\\rho\\right)\\\\\n",
    " & \\ge J\\left(g\\right),\n",
    "\\end{align*}\n",
    "\n",
    "and the equality holds only when:\n",
    "\n",
    "$$\\langle\\rho,\\rho\\rangle_{\\mathcal{H}_{K}}=0\\Rightarrow\\rho=0.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
