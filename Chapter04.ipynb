{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.1\n",
    "\n",
    "This is a generalized Rayleigh Quotient. $\\mathbf{W}$ is the within-class covariance matrix so it is a Hermitian positive-definite matrix, almost surely. There is an unique Cholesky decomposition of  $\\mathbf{W}$:\n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{C}\\mathbf{C}^T, $$\n",
    "where $\\mathbf{C}$ is a lower triangular matrix.\n",
    "\n",
    "Let's denote, $\\mathbf{A}=\\mathbf{C}^{-1}B\\mathbf{C}^{-T}$, then the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & a^{T}\\mathbf{CAC}^{T}a\\\\\n",
    "\\mbox{subject to}\\quad & a^{T}\\mathbf{CC}^{T}a=1\n",
    "\\end{align*}\n",
    "\n",
    "If we define $y=\\mathbf{C}^{T}a$, the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & y^{T}\\mathbf{A}y\\\\\n",
    "\\mbox{subject to}\\quad & y^{T}y=1\n",
    "\\end{align*}\n",
    "\n",
    "This is an ordinary Rayleigh Quotient, and its maximum is the largest eigenvalue of $\\mathbf{A}$. The transfer is valid because $\\mathbf{C}$ is full rank and for any $y$ there exsit a unique $a$ such that $y=\\mathbf{C}^{T}a$. In summery, by using the Cholesky decomposition of $\\mathbf{W}$, we can transfer this problem to a standard eigenvalue problem.\n",
    "\n",
    "A few rearrangement can bring us another ordinary eigenvalue problem that can also solve this problem. Let's denote the solution, i.e. that largest eigenvalue of $\\mathbf{A}$ as $\\lambda$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{AC}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{BC}^{-T}\\mathbf{C}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-T}\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a.\n",
    "\\end{align*}\n",
    "\n",
    "We can see that $\\lambda$ is also an eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. When the above condition is satisfied, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "a\\mathbf{B}a & =a\\mathbf{WW}^{-1}\\mathbf{B}a\\\\\n",
    " & =\\lambda a\\mathbf{W}a\\\\\n",
    " & =\\lambda.\n",
    "\\end{align*}\n",
    "\n",
    "This shows us that we can also solve the original problem by finding the largest eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. In summery, the solution is the largest eigenvalue of both $\\mathbf{C}^{-1}\\mathbf{BC}^{-T}$ and $\\mathbf{W}^{-1}\\mathbf{B}$, with correspoding eigenvector $\\mathbf{C}^{T}a$ and $a$, respectively. \n",
    "\n",
    "It is also interesting to show how this is related to lagrangian multiplier. $\\lambda$ being eigenvector of $\\mathbf{W}^{-1}\\mathbf{B}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{B}a & =\\lambda\\mathbf{W}a\\\\\n",
    "\\Rightarrow\\nabla\\left(a\\mathbf{B}a\\right) & =\\lambda\\nabla\\left(a\\mathbf{W}a\\right),\n",
    "\\end{align*}\n",
    "which is the stationary condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.2\n",
    "## (a)\n",
    "\n",
    "Consider the frequency of class 1 and 2 in the training set as the prior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{1} & =\\frac{N_{1}}{N},\\\\\n",
    "\\pi_{2} & =\\frac{N_{2}}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "The posterior density is proportional to the likelihood times prior:\n",
    "\n",
    "\\begin{align*}\n",
    "p(1|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}}\\pi_{1},\\\\\n",
    "p(2|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}}\\pi_{2}.\n",
    "\\end{align*}\n",
    "\n",
    "A point $x$ is classified to class 2 if: $\\log p(2|x) > \\log p(1|x)$, that is:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}+\\log\\frac{N_{2}}{N} & >-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}+\\log\\frac{N_{1}}{N}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\hat{\\mu_{2}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{2}}-\\frac{1}{2}\\hat{\\mu_{1}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{1}}-\\log\\frac{N_{2}}{N_{1}}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\left(\\hat{\\mu_{2}}+\\hat{\\mu_{1}}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}\\right)-\\log\\frac{N_{2}}{N_{1}}\n",
    "\\end{align*}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
