{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.1\n",
    "\n",
    "This is a generalized Rayleigh Quotient. $\\mathbf{W}$ is the within-class covariance matrix so it is a Hermitian positive-definite matrix, almost surely. There is an unique Cholesky decomposition of  $\\mathbf{W}$:\n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{C}\\mathbf{C}^T, $$\n",
    "where $\\mathbf{C}$ is a lower triangular matrix.\n",
    "\n",
    "Let's denote, $\\mathbf{A}=\\mathbf{C}^{-1}B\\mathbf{C}^{-T}$, then the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & a^{T}\\mathbf{CAC}^{T}a\\\\\n",
    "\\mbox{subject to}\\quad & a^{T}\\mathbf{CC}^{T}a=1\n",
    "\\end{align*}\n",
    "\n",
    "If we define $y=\\mathbf{C}^{T}a$, the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & y^{T}\\mathbf{A}y\\\\\n",
    "\\mbox{subject to}\\quad & y^{T}y=1\n",
    "\\end{align*}\n",
    "\n",
    "This is an ordinary Rayleigh Quotient, and its maximum is the largest eigenvalue of $\\mathbf{A}$. The transfer is valid because $\\mathbf{C}$ is full rank and for any $y$ there exsit a unique $a$ such that $y=\\mathbf{C}^{T}a$. In summery, by using the Cholesky decomposition of $\\mathbf{W}$, we can transfer this problem to a standard eigenvalue problem.\n",
    "\n",
    "A few rearrangement can bring us another ordinary eigenvalue problem that can also solve this problem. Let's denote the solution, i.e. that largest eigenvalue of $\\mathbf{A}$ as $\\lambda$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{AC}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{BC}^{-T}\\mathbf{C}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-T}\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a.\n",
    "\\end{align*}\n",
    "\n",
    "We can see that $\\lambda$ is also an eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. When the above condition is satisfied, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "a\\mathbf{B}a & =a\\mathbf{WW}^{-1}\\mathbf{B}a\\\\\n",
    " & =\\lambda a\\mathbf{W}a\\\\\n",
    " & =\\lambda.\n",
    "\\end{align*}\n",
    "\n",
    "This shows us that we can also solve the original problem by finding the largest eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. In summery, the solution is the largest eigenvalue of both $\\mathbf{C}^{-1}\\mathbf{BC}^{-T}$ and $\\mathbf{W}^{-1}\\mathbf{B}$, with correspoding eigenvector $\\mathbf{C}^{T}a$ and $a$, respectively. \n",
    "\n",
    "It is also interesting to show how this is related to lagrangian multiplier. $\\lambda$ being eigenvector of $\\mathbf{W}^{-1}\\mathbf{B}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{B}a & =\\lambda\\mathbf{W}a\\\\\n",
    "\\Rightarrow\\nabla\\left(a\\mathbf{B}a\\right) & =\\lambda\\nabla\\left(a\\mathbf{W}a\\right),\n",
    "\\end{align*}\n",
    "which is the stationary condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.2\n",
    "## (a)\n",
    "\n",
    "Consider the frequency of class 1 and 2 in the training set as the prior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{1} & =\\frac{N_{1}}{N},\\\\\n",
    "\\pi_{2} & =\\frac{N_{2}}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "The posterior density is proportional to the likelihood times prior:\n",
    "\n",
    "\\begin{align*}\n",
    "p(1|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}}\\pi_{1},\\\\\n",
    "p(2|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}}\\pi_{2}.\n",
    "\\end{align*}\n",
    "\n",
    "A point $x$ is classified to class 2 if: $\\log p(2|x) > \\log p(1|x)$, that is:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}+\\log\\frac{N_{2}}{N} & >-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}+\\log\\frac{N_{1}}{N}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\hat{\\mu_{2}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{2}}-\\frac{1}{2}\\hat{\\mu_{1}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{1}}-\\log\\frac{N_{2}}{N_{1}}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\left(\\hat{\\mu_{2}}+\\hat{\\mu_{1}}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}\\right)-\\log\\frac{N_{2}}{N_{1}}\n",
    "\\end{align*}\n",
    "\n",
    "## (b)\n",
    "Let's denote $1_N$ as the $N\\times 1$ column vector with all entries being $1$. Let $\\mathbf{X}_1$ be defined as the $N_1\\times p$ matrix of training set in class $1$, and $\\mathbf{X}_2$ be defined similarly. Then centroids for class $1$ and $2$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mu}_{1} & =\\frac{1}{N_{1}}\\mathbf{X}_{1}^{T}\\cdot1_{N_{1}},\\\\\n",
    "\\hat{\\mu}_{2} & =\\frac{1}{N_{2}}\\mathbf{X}_{2}^{T}\\cdot1_{N_{2}}.\n",
    "\\end{align*}\n",
    "\n",
    "The normal equation $\\beta_0$ and $\\beta$ satisfy is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right] & =\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}y,\n",
    "\\end{align*}\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "1_{N} & =\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right],\\\\\n",
    "\\mathbf{X} & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right],\\\\\n",
    "y & =\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Notice that:\n",
    "\n",
    "\\begin{align*}\n",
    "1_{N}^{T}y & =\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =-1_{N_{1}}^{T}\\frac{N}{N_{1}}1_{N_{1}}+1_{N_{2}}^{T}\\frac{N}{N_{2}}1_{N_{2}}\\\\\n",
    " & =-\\frac{N}{N_{1}}N_{1}+\\frac{N}{N_{2}}N_{2}\\\\\n",
    " & =0,\n",
    "\\end{align*}\n",
    "and that:\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "The noraml equation can be rewritten as:\n",
    "$$\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "X^{T}y\n",
    "\\end{array}\\right]^{T}.$$\n",
    "\n",
    "We can solve for $\\beta_0$ by the first line of the above system:\n",
    "$$\\beta_{0}=-\\frac{1}{N}1_{N}^{T}X\\beta,$$\n",
    "and plug it into the rest equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta & =X^{T}y\\\\\n",
    " & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =-\\frac{N}{N_{1}}\\mathbf{X}_{1}^{T}1_{N_{1}}+\\frac{N}{N_{2}}\\mathbf{X}_{2}^{T}1_{N_{2}}\\\\\n",
    " & =-\\frac{N}{N_{1}}N_{1}\\hat{\\mu}_{1}+\\frac{N}{N_{2}}N_{2}\\hat{\\mu}_{2}\\\\\n",
    " & =N\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "The RHS is already done, let's continue on the LHS. Observe that:\n",
    "\n",
    "\\begin{align*}\n",
    "X^{T}1_{N} & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =\\mathbf{X}_{1}^{T}1_{N_{1}}+\\mathbf{X}_{2}^{T}1_{N_{2}}\\\\\n",
    " & =N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}.\n",
    "\\end{align*}\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X & =\\frac{1}{N}\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)^{T}\\\\\n",
    " & =\\frac{1}{N}\\left(N_{1}^{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{2}^{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)\n",
    "\\end{align*}\n",
    "In addition,\n",
    "\n",
    "\\begin{align*}\n",
    "(N-2)\\hat{\\Sigma} & =\\left(\\mathbf{X}_{1}-1_{N_{1}}\\hat{\\mu}_{1}^{T}\\right)^{T}\\left(\\mathbf{X}_{1}-1_{N_{1}}\\hat{\\mu}_{1}^{T}\\right)+\\left(\\mathbf{X}_{2}-1_{N_{2}}\\hat{\\mu}_{2}^{T}\\right)^{T}\\left(\\mathbf{X}_{2}-1_{N_{2}}\\hat{\\mu}_{2}^{T}\\right)\\\\\n",
    " & =\\mathbf{X}_{1}^{T}\\mathbf{X}_{1}+\\mathbf{X}_{2}^{T}\\mathbf{X}_{2}-2\\mathbf{X}_{1}^{T}1_{N_{1}}\\hat{\\mu}_{1}^{T}+\\hat{\\mu}_{1}1_{N_{1}}^{T}1_{N_{1}}\\hat{\\mu}_{1}^{T}-2\\mathbf{X}_{2}^{T}1_{N_{2}}\\hat{\\mu}_{2}^{T}+\\hat{\\mu}_{2}1_{N_{2}}^{T}1_{N_{2}}\\hat{\\mu}_{2}^{T}\\\\\n",
    " & =\\mathbf{X}^{T}\\mathbf{X}-N_{1}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}.\n",
    "\\end{align*}\n",
    "\n",
    "The LHS of the normal equation can be reorganized as:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta\\\\\n",
    " & =\\left(-\\frac{1}{N}\\left(N_{1}^{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{2}^{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+N_{1}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{1}{N}\\left(\\left(N_{1}N-N_{1}^{2}\\right)\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+\\left(N_{2}N-N_{2}^{2}\\right)\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{1}{N}\\left(N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{1}N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{N_{1}N_{2}}{N}\\left(\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{N_{1}N_{2}}{N}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(N\\hat{\\Sigma}_{B}+(N-2)\\hat{\\Sigma}\\right)\\beta.\n",
    "\\end{align*}\n",
    "\n",
    "This completes the proof."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
