{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.1\n",
    "\n",
    "This is a generalized Rayleigh Quotient. $\\mathbf{W}$ is the within-class covariance matrix so it is a Hermitian positive-definite matrix, almost surely. There is an unique Cholesky decomposition of  $\\mathbf{W}$:\n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{C}\\mathbf{C}^T, $$\n",
    "where $\\mathbf{C}$ is a lower triangular matrix.\n",
    "\n",
    "Let's denote, $\\mathbf{A}=\\mathbf{C}^{-1}B\\mathbf{C}^{-T}$, then the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & a^{T}\\mathbf{CAC}^{T}a\\\\\n",
    "\\mbox{subject to}\\quad & a^{T}\\mathbf{CC}^{T}a=1\n",
    "\\end{align*}\n",
    "\n",
    "If we define $y=\\mathbf{C}^{T}a$, the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & y^{T}\\mathbf{A}y\\\\\n",
    "\\mbox{subject to}\\quad & y^{T}y=1\n",
    "\\end{align*}\n",
    "\n",
    "This is an ordinary Rayleigh Quotient, and its maximum is the largest eigenvalue of $\\mathbf{A}$. The transfer is valid because $\\mathbf{C}$ is full rank and for any $y$ there exsit a unique $a$ such that $y=\\mathbf{C}^{T}a$. In summery, by using the Cholesky decomposition of $\\mathbf{W}$, we can transfer this problem to a standard eigenvalue problem.\n",
    "\n",
    "A few rearrangement can bring us another ordinary eigenvalue problem that can also solve this problem. Let's denote the solution, i.e. that largest eigenvalue of $\\mathbf{A}$ as $\\lambda$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{AC}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{BC}^{-T}\\mathbf{C}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-T}\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a.\n",
    "\\end{align*}\n",
    "\n",
    "We can see that $\\lambda$ is also an eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. When the above condition is satisfied, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "a\\mathbf{B}a & =a\\mathbf{WW}^{-1}\\mathbf{B}a\\\\\n",
    " & =\\lambda a\\mathbf{W}a\\\\\n",
    " & =\\lambda.\n",
    "\\end{align*}\n",
    "\n",
    "This shows us that we can also solve the original problem by finding the largest eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. In summery, the solution is the largest eigenvalue of both $\\mathbf{C}^{-1}\\mathbf{BC}^{-T}$ and $\\mathbf{W}^{-1}\\mathbf{B}$, with correspoding eigenvector $\\mathbf{C}^{T}a$ and $a$, respectively. \n",
    "\n",
    "It is also interesting to show how this is related to lagrangian multiplier. $\\lambda$ being eigenvector of $\\mathbf{W}^{-1}\\mathbf{B}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{B}a & =\\lambda\\mathbf{W}a\\\\\n",
    "\\Rightarrow\\nabla\\left(a\\mathbf{B}a\\right) & =\\lambda\\nabla\\left(a\\mathbf{W}a-1\\right),\n",
    "\\end{align*}\n",
    "which is the stationary condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.2\n",
    "## (a)\n",
    "\n",
    "Consider the frequency of class 1 and 2 in the training set as the prior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{1} & =\\frac{N_{1}}{N},\\\\\n",
    "\\pi_{2} & =\\frac{N_{2}}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "The posterior density is proportional to the likelihood times prior:\n",
    "\n",
    "\\begin{align*}\n",
    "p(1|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}}\\pi_{1},\\\\\n",
    "p(2|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}}\\pi_{2}.\n",
    "\\end{align*}\n",
    "\n",
    "A point $x$ is classified to class 2 if: $\\log p(2|x) > \\log p(1|x)$, that is:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}+\\log\\frac{N_{2}}{N} & >-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}+\\log\\frac{N_{1}}{N}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\hat{\\mu_{2}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{2}}-\\frac{1}{2}\\hat{\\mu_{1}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{1}}-\\log\\frac{N_{2}}{N_{1}}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\left(\\hat{\\mu_{2}}+\\hat{\\mu_{1}}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}\\right)-\\log\\frac{N_{2}}{N_{1}}\n",
    "\\end{align*}\n",
    "\n",
    "## (b)\n",
    "Let's denote $1_N$ as the $N\\times 1$ column vector with all entries being $1$. Let $\\mathbf{X}_1$ be defined as the $N_1\\times p$ matrix of training set in class $1$, and $\\mathbf{X}_2$ be defined similarly. Then centroids for class $1$ and $2$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mu}_{1} & =\\frac{1}{N_{1}}\\mathbf{X}_{1}^{T}\\cdot1_{N_{1}},\\\\\n",
    "\\hat{\\mu}_{2} & =\\frac{1}{N_{2}}\\mathbf{X}_{2}^{T}\\cdot1_{N_{2}}.\n",
    "\\end{align*}\n",
    "\n",
    "The normal equation $\\beta_0$ and $\\beta$ satisfy is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right] & =\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}y,\n",
    "\\end{align*}\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "1_{N} & =\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right],\\\\\n",
    "\\mathbf{X} & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right],\\\\\n",
    "y & =\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Notice that:\n",
    "\n",
    "\\begin{align*}\n",
    "1_{N}^{T}y & =\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =-1_{N_{1}}^{T}\\frac{N}{N_{1}}1_{N_{1}}+1_{N_{2}}^{T}\\frac{N}{N_{2}}1_{N_{2}}\\\\\n",
    " & =-\\frac{N}{N_{1}}N_{1}+\\frac{N}{N_{2}}N_{2}\\\\\n",
    " & =0,\n",
    "\\end{align*}\n",
    "and that:\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "The noraml equation can be rewritten as:\n",
    "$$\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "X^{T}y\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "We can solve for $\\beta_0$ by the first line of the above system:\n",
    "$$\\beta_{0}=-\\frac{1}{N}1_{N}^{T}X\\beta,$$\n",
    "and plug it into the rest equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta & =X^{T}y\\\\\n",
    " & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =-\\frac{N}{N_{1}}\\mathbf{X}_{1}^{T}1_{N_{1}}+\\frac{N}{N_{2}}\\mathbf{X}_{2}^{T}1_{N_{2}}\\\\\n",
    " & =-\\frac{N}{N_{1}}N_{1}\\hat{\\mu}_{1}+\\frac{N}{N_{2}}N_{2}\\hat{\\mu}_{2}\\\\\n",
    " & =N\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "The RHS is already done, let's continue on the LHS. Observe that:\n",
    "\n",
    "\\begin{align*}\n",
    "X^{T}1_{N} & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =\\mathbf{X}_{1}^{T}1_{N_{1}}+\\mathbf{X}_{2}^{T}1_{N_{2}}\\\\\n",
    " & =N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}.\n",
    "\\end{align*}\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X & =\\frac{1}{N}\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)^{T}\\\\\n",
    " & =\\frac{1}{N}\\left(N_{1}^{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{2}^{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)\n",
    "\\end{align*}\n",
    "In addition,\n",
    "\n",
    "\\begin{align*}\n",
    "(N-2)\\hat{\\Sigma} & =\\left(\\mathbf{X}_{1}-1_{N_{1}}\\hat{\\mu}_{1}^{T}\\right)^{T}\\left(\\mathbf{X}_{1}-1_{N_{1}}\\hat{\\mu}_{1}^{T}\\right)+\\left(\\mathbf{X}_{2}-1_{N_{2}}\\hat{\\mu}_{2}^{T}\\right)^{T}\\left(\\mathbf{X}_{2}-1_{N_{2}}\\hat{\\mu}_{2}^{T}\\right)\\\\\n",
    " & =\\mathbf{X}_{1}^{T}\\mathbf{X}_{1}+\\mathbf{X}_{2}^{T}\\mathbf{X}_{2}-2\\mathbf{X}_{1}^{T}1_{N_{1}}\\hat{\\mu}_{1}^{T}+\\hat{\\mu}_{1}1_{N_{1}}^{T}1_{N_{1}}\\hat{\\mu}_{1}^{T}-2\\mathbf{X}_{2}^{T}1_{N_{2}}\\hat{\\mu}_{2}^{T}+\\hat{\\mu}_{2}1_{N_{2}}^{T}1_{N_{2}}\\hat{\\mu}_{2}^{T}\\\\\n",
    " & =\\mathbf{X}^{T}\\mathbf{X}-N_{1}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}.\n",
    "\\end{align*}\n",
    "\n",
    "The LHS of the normal equation can be reorganized as:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta\\\\\n",
    " & =\\left(-\\frac{1}{N}\\left(N_{1}^{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{2}^{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+N_{1}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{1}{N}\\left(\\left(N_{1}N-N_{1}^{2}\\right)\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+\\left(N_{2}N-N_{2}^{2}\\right)\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{1}{N}\\left(N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{1}N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{N_{1}N_{2}}{N}\\left(\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{N_{1}N_{2}}{N}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(N\\hat{\\Sigma}_{B}+(N-2)\\hat{\\Sigma}\\right)\\beta.\n",
    "\\end{align*}\n",
    "\n",
    "This completes the proof.\n",
    "\n",
    "## (c)\n",
    "$\\hat{\\Sigma}_{B}\\beta$ is in direction of $\\hat{\\mu}_{2}-\\hat{\\mu}_{1}$, becuase it is a scaler times $\\hat{\\mu}_{2}-\\hat{\\mu}_{1}$, where the scaler is:\n",
    "$$\\frac{N_{1}N_{2}}{N^2}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}\\beta.$$\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\left(N-2\\right)\\hat{\\Sigma}\\beta & =N\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)-N\\hat{\\Sigma}_{B}\\beta\\\\\n",
    "\\Rightarrow\\hat{\\Sigma}\\beta & =\\frac{N}{N-2}\\left(\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)-\\hat{\\Sigma}_{B}\\beta\\right)\\\\\n",
    "\\Rightarrow\\beta & =\\frac{N}{N-2}\\hat{\\Sigma}^{-1}\\left(\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)-\\hat{\\Sigma}_{B}\\beta\\right)\\\\\n",
    " & =\\frac{N}{N-2}\\left(1-\\frac{N_{1}N_{2}}{N^{2}}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}\\beta\\right)\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\\\\\n",
    "\\Rightarrow\\beta & \\propto\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "As shown in (a), the normal vector of the affine decision boundary of LDA is:\n",
    "$\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right).$\n",
    "\n",
    "## (d)\n",
    "Lets code the value of class $1$ and $2$ as $k_1$ and $k_2$ respectively. The normal equation becomes:\n",
    "\n",
    "$\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "1_{N}^{T}y\\\\\n",
    "X^{T}y\n",
    "\\end{array}\\right].$\n",
    "\n",
    "In this case, $1_{N}^{T}y=N_{1}k_{1}+N_{2}k_{2}$ , and $X^{T}y=N_{1}k_{1}\\hat{\\mu}_{1}+N_{2}k_{2}\\hat{\\mu}_{2}$. After eliminating $\\beta_0$ folliwng the same approach as in (b), we have:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta\\\\\n",
    " & =X^{T}y-\\frac{1}{N}\\mathbf{X}^{T}1_{N}1_{N}^{T}y\\\\\n",
    " & =N_{1}k_{1}\\hat{\\mu}_{1}+N_{2}k_{2}\\hat{\\mu}_{2}-\\frac{1}{N}\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)\\left(N_{1}k_{1}+N_{2}k_{2}\\right)\\\\\n",
    " & =N_{2}\\hat{\\mu}_{2}\\left(k_{2}-\\frac{N_{1}k_{1}+N_{2}k_{2}}{N}\\right)-N_{1}\\hat{\\mu}_{1}\\left(\\frac{N_{1}k_{1}+N_{2}k_{2}}{N}-k_{1}\\right)\\\\\n",
    " & =\\frac{N_{1}N_{2}}{N}\\hat{\\mu}_{2}\\left(k_{2}-k_{1}\\right)-\\frac{N_{1}N_{2}}{N}\\hat{\\mu}_{1}\\left(k_{2}-k_{1}\\right)\\\\\n",
    " & =\\frac{N_{1}N_{2}}{N}\\left(k_{2}-k_{1}\\right)\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Compared with previous case, the LHS stays the same, and the RHS is only different up to a scalar multiple. So the property in (c) still holds.\n",
    "\n",
    "## (e)\n",
    "\n",
    "Let's first prove that when $N_1 = N_2$, LDA and least square have the same decission boundary. The boundary for LDA is:\n",
    "$$x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}})=\\frac{1}{2}\\left(\\hat{\\mu_{2}}+\\hat{\\mu_{1}}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}\\right).$$\n",
    "\n",
    "For linear least square with class coding in (a) and $N_1 = N_2$, the decision boundary is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f}(x)=\\hat{\\beta_{0}}+x^{T}\\hat{\\beta} & =0\\\\\n",
    "\\Rightarrow\\left(x^{T}-\\frac{1}{N}1_{N}^{T}X\\right)\\hat{\\beta} & =0\\\\\n",
    "\\Rightarrow\\left(x-\\left(\\frac{N_{1}}{N}\\hat{\\mu}_{1}+\\frac{N_{2}}{N}\\hat{\\mu}_{2}\\right)\\right)^{T}\\hat{\\beta} & =0\\\\\n",
    "\\Rightarrow\\left(x-\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)\\right)^{T}\\hat{\\beta} & =0\n",
    "\\end{align*}\n",
    "\n",
    "From (c) we know that\n",
    "$$\\hat{\\beta}=\\lambda\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right), $$\n",
    "where $\\lambda$ is a scalar. If we plug it into the previous equation we obtain the decision boundary:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(x-\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)\\right)^{T}\\lambda\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right) & =0\\\\\n",
    "\\Rightarrow\\left(x-\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right) & =0\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right) & =\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We can see the decision boundary in these two methods are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.3\n",
    "\n",
    "The solution for this problem can be found at the following two papers:\n",
    "\n",
    "Hastie, Tibshirani, and Buja. 1994, flexible discriminant analysis by optimal scoring. Journal of the American Statistical Association.\n",
    "\n",
    "Breiman and Ihaka. 1984, Nolinear discriminant analysis via scaling and ACE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 4.6\n",
    "### (a)\n",
    "For any point $x$, it is classified to $1$ if $f(x) = \\beta^Tx^* > 0 $ and to $-1$ if $f(x) = \\beta^Tx^* < 0 $. As we can see, for correctly classified points, we have:\n",
    "$yf(x) >0,$\n",
    "and for misclassified points, we have:\n",
    "$yf(x) < 0.$\n",
    "\n",
    "Suppose those $N$ points are linearly separable and $\\beta$ defines such a seperating hyperplane, then we have:\n",
    "\n",
    "\\begin{align*}\n",
    "y_{i}\\beta^{T}x_{i}^{*} & >0,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "y_{i}\\beta^{T}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & >0,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "\\exists C>0\\,\\mbox{s. t. }y_{i}\\beta^{T}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & \\ge C,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "\\exists C>0\\,\\mbox{s. t. }y_{i}\\frac{\\beta^{T}}{C}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & \\ge1,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "\\exists\\beta_{\\mbox{sep}}^{T}=\\frac{\\beta^{T}}{C}\\,\\mbox{s. t. }y_{i}\\beta_{\\mbox{sep}}^{T}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & \\ge1,\\quad\\forall i\n",
    "\\end{align*}\n",
    "\n",
    "### (b)\n",
    "It will be less confusing to rephrase the perceptron algorithm in this way. Suppose $x_i$ is misclassified, then update $\\beta$ by:\n",
    "$$\\beta_{\\mbox{new}} \\leftarrow\\beta_{\\mbox{old}}+\\rho y_{i}x_{i}^{*},$$\n",
    "where $\\rho=1/\\left\\Vert x_i^*\\right\\Vert.$\n",
    "\n",
    "Let $\\beta_{\\mbox{sep}}$ be defined exactly the same as in (a), we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{\\mbox{new}} & =\\beta_{\\mbox{old}}+y_{i}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert }=\\beta_{\\mbox{old}}+y_{i}z_{i}\\,\\Rightarrow\\\\\n",
    "\\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}} & =\\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}+y_{i}z_{i}\\,\\Rightarrow\\\\\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}+y_{i}z_{i}\\right\\Vert ^{2}\\,\\Rightarrow\\\\\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2}+2y_{i}\\left(\\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right)^{T}z_{i}+y_{i}^{2}\\left\\Vert z_{i}\\right\\Vert ^{2}\\,\\Rightarrow\\\\\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2}+2\\left(y_{i}\\beta_{\\mbox{old}}^{T}z_{i}-y_{i}\\beta_{\\mbox{sep}}^{T}z_{i}\\right)+1.\n",
    "\\end{align*}\n",
    "\n",
    "Since $x_i$ is misclassified by $\\beta_{\\mbox{old}}$, we have: $y_i\\beta_{\\mbox{old}}^Tx_i < 0$. We just proved in (a) that $y_{i}\\beta_{\\mbox{sep}}^{T}z_{i} \\ge 1$, which leads to \n",
    "$$2\\left(y_{i}\\beta_{\\mbox{old}}^{T}z_{i}-y_{i}\\beta_{\\mbox{sep}}^{T}z_{i}\\right) < -2,$$ and finally we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2}-\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =2\\left(y_{i}\\beta_{\\mbox{old}}^{T}z_{i}-y_{i}\\beta_{\\mbox{sep}}^{T}z_{i}\\right)+1\\\\\n",
    " & \\le-2+1\\\\\n",
    " & =-1.\n",
    "\\end{align*}\n",
    "This completes the proof."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
