{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.1\n",
    "\n",
    "First we need to assume that the norm here is Euclidean norm $||\\cdot||_2$, we also believe this is the case for any norm appearing in this book. The statement in this problem is true under Euclidean norm, but not necessarily true for other norms. Let's consider two vectors in $v_1, v_2 \\in \\mathbb{R}^2 $, where $ v_1 = [4,4]^T, v_2 = [6,1]^T $. We can see that:\n",
    "\\begin{align*}\n",
    "  ||v_1||_1 >& \\,||v_2||_1 \\\\ \n",
    "  ||v_1||_2 <& \\,||v_2||_2 .\n",
    "\\end{align*}\n",
    "\n",
    "This indicates that for a given point, the nearest neighbor is not invariant with different choices of norms.\n",
    "  \n",
    "Let's come back to prove the statment in this question:\n",
    "$$\\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2\\right) = \\mathrm{argmax}_k\\left(\\hat{y}_k\\right)$$\n",
    "**Proof**\n",
    "\\begin{align*}\n",
    "  \\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2\\right) \n",
    "  =&\\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2^2\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\sum_{i=1}^K\\left( t_{k\\,i} - \\hat{y}_i\\right)^2 \\\\\n",
    "  =&\\mathrm{argmin}_k\\sum_{i=1}^K\\left(t_{k\\,i}^2 + \\hat{y}_i^2 - 2t_{k\\,i}\\hat{y}_i\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\left(1+\\left\\Vert\\hat{y}\\right\\Vert_2^2-2\\sum_{i=1}^Kt_{k\\,i}\\hat{y}_i\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\left(1+\\left\\Vert\\hat{y}\\right\\Vert_2^2-2\\hat{y}_k\\right) \\\\\n",
    "  =&\\mathrm{argmin}_k\\left(-2\\hat{y}_k\\right) \\\\\n",
    "  =&\\mathrm{argmax}_k\\left(\\hat{y}_k\\right)\n",
    "\\end{align*}âˆŽ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.2\n",
    "\n",
    "The Bayes decision boundary is atainable because we know how the two classes (Blue and Organge) are generated. Let's first review on how these two classes are generated. We define a Gaussian bivariate distribution for each of the two classes, with the covariance matrix being the same $2\\times2$ identity matrix $\\mathbf{I}$, and mean vectors different. For the Blue class, the mean vector is $[1,0]^T$, and for the Organge $[0,1]^T$. Namely $\\mathbf{Z}_1 = N\\left([1,0]^T, \\mathbf{I}\\right), \\mathbf{Z}_2 = N\\left([0,1]^T, \\mathbf{I}\\right)$. For each of the two classes, we sample 10 times from its corresponding bivariate Gaussian vector. We denote the two set of samples as: $\\mathbf{p}=\\{p_i\\}_{i=1}^{10}$ for the Blue and $\\mathbf{q}=\\{q_i\\}_{i=1}^{10}$ for the Orange. Next, for each of the two classes, we first uniformly pick an element form $\\mathbf{p}$ (or $\\mathbf{q}$) to be the mean vector, and use $\\frac15\\mathbf{I}$ as the covariance matrix to form another bivariate Gaussian vector to generate a single realization. Such process is repeated for 100 times for each class.\n",
    "\n",
    "Now let's first look for the Bayes decision boundary conditional on $\\mathbf{p}$ and $\\mathbf{q}$, $\\Gamma(\\mathbf{p}, \\mathbf{q})\\in \\mathbb{R}^2$. For any $x\\in\\mathbb{R}^2$. The probability of $x$ being blue is:\n",
    "$$P(x\\,\\mathrm{being\\,blue})= \\sum_{i=1}^{10}\\frac1{10}\\phi(x,p_i), $$\n",
    "where $\\phi(x,p_i)$ is the probability density function at $x$ of a bivariate Guassian with mean $p_i$ and covariance matrix $\\frac15\\mathbf{I}$. Then the Bayes decision boundary conditional on $\\mathbf{p}$ and $\\mathbf{q}$ should be those points where being blue and orange are equally likely:\n",
    "$$\\Gamma(\\mathbf{p}, \\mathbf{q})\n",
    "=\\{x\\in\\mathbb{R}^2\\vert \\sum_{i=1}^{10}\\phi(x, p_i)=\\sum_{i=1}^{10}\\phi(x, q_i)\\}$$\n",
    "\n",
    "The final Bayes decision boundary should only be conditinal on given paramters $\\mathbf{Z}_1$ and $\\mathbf{Z}_2$, which should be the expectation of $\\Gamma(\\mathbf{p}, \\mathbf{q})$ over all possible $\\mathbf{p}$ or $\\mathbf{q}$:\n",
    "\\begin{align*}\n",
    "\\Gamma(\\mathbf{Z}_1, \\mathbf{Z}_2) =& \\Gamma(\\mathbb{E}[\\mathbf{p}],\\mathbb{E}[\\mathbf{q}])\\\\\n",
    "=&\\{x \\in \\mathbb{R}^2\\,\\vert\\,\\phi(x,[1,0]^T)=\\phi(x,[0,1]^T)\\}\\\\\n",
    "=&\\{x \\in \\mathbb{R}^2\\,\\vert\\, x_1 = x_2\\}.\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
