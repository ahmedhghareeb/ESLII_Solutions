{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.1\n",
    "\n",
    "First we need to assume that the norm here is Euclidean norm $||\\cdot||_2$, we also believe this is the case for any norm appearing in this book. The statement in this problem is true under Euclidean norm, but not necessarily true for other norms. Let's consider two vectors in $v_1, v_2 \\in \\mathbb{R}^2 $, where $ v_1 = [4,4]^T, v_2 = [6,1]^T $. We can see that:\n",
    "\\begin{align*}\n",
    "  ||v_1||_1 >& \\,||v_2||_1 \\\\ \n",
    "  ||v_1||_2 <& \\,||v_2||_2 .\n",
    "\\end{align*}\n",
    "\n",
    "This indicates that for a given point, the nearest neighbor is not invariant with different choices of norms.\n",
    "  \n",
    "Let's come back to prove the statment in this question:\n",
    "$$\\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2\\right) = \\mathrm{argmax}_k\\left(\\hat{y}_k\\right)$$\n",
    "**Proof**\n",
    "\\begin{align*}\n",
    "  \\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2\\right) \n",
    "  =&\\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2^2\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\sum_{i=1}^K\\left( t_{k\\,i} - \\hat{y}_i\\right)^2 \\\\\n",
    "  =&\\mathrm{argmin}_k\\sum_{i=1}^K\\left(t_{k\\,i}^2 + \\hat{y}_i^2 - 2t_{k\\,i}\\hat{y}_i\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\left(1+\\left\\Vert\\hat{y}\\right\\Vert_2^2-2\\sum_{i=1}^Kt_{k\\,i}\\hat{y}_i\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\left(1+\\left\\Vert\\hat{y}\\right\\Vert_2^2-2\\hat{y}_k\\right) \\\\\n",
    "  =&\\mathrm{argmin}_k\\left(-2\\hat{y}_k\\right) \\\\\n",
    "  =&\\mathrm{argmax}_k\\left(\\hat{y}_k\\right)\n",
    "\\end{align*}âˆŽ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.2\n",
    "\n",
    "The Bayes decision boundary is atainable because we know how the two classes (Blue and Organge) are generated. Let's first review on how these two classes are generated. We define a Gaussian bivariate distribution for each of the two classes, with the covariance matrix being the same $2\\times2$ identity matrix $\\mathbf{I}$, and mean vectors different. For the Blue class, the mean vector is $[1,0]^T$, and for the Organge $[0,1]^T$. Namely $\\mathbf{Z}_1 = N\\left([1,0]^T, \\mathbf{I}\\right), \\mathbf{Z}_2 = N\\left([0,1]^T, \\mathbf{I}\\right)$. For each of the two classes, we sample 10 times from its corresponding bivariate Gaussian vector. We denote the two set of samples as: $\\mathbf{p}=\\{p_i\\}_{i=1}^{10}$ for the Blue and $\\mathbf{q}=\\{q_i\\}_{i=1}^{10}$ for the Orange. Next, for each of the two classes, we first uniformly pick an element form $\\mathbf{p}$ (or $\\mathbf{q}$) to be the mean vector, and use $\\frac15\\mathbf{I}$ as the covariance matrix to form another bivariate Gaussian vector to generate a single realization. Such process is repeated for 100 times for each class.\n",
    "\n",
    "Now let's first look for the Bayes decision boundary conditional on $\\mathbf{p}$ and $\\mathbf{q}$, $\\Gamma(\\mathbf{p}, \\mathbf{q})\\in \\mathbb{R}^2$. For any $x\\in\\mathbb{R}^2$. The probability of $x$ being blue is:\n",
    "$$P(x\\,\\mathrm{being\\,blue})= \\sum_{i=1}^{10}\\frac1{10}\\phi(x,p_i), $$\n",
    "where $\\phi(x,p_i)$ is the probability density function at $x$ of a bivariate Guassian with mean $p_i$ and covariance matrix $\\frac15\\mathbf{I}$. Then the Bayes decision boundary conditional on $\\mathbf{p}$ and $\\mathbf{q}$ should be those points where being blue and orange are equally likely:\n",
    "$$\\Gamma(\\mathbf{p}, \\mathbf{q})\n",
    "=\\{x\\in\\mathbb{R}^2\\vert \\sum_{i=1}^{10}\\phi(x, p_i)=\\sum_{i=1}^{10}\\phi(x, q_i)\\}$$\n",
    "\n",
    "The final Bayes decision boundary should only be conditinal on given paramters $\\mathbf{Z}_1$ and $\\mathbf{Z}_2$, which should be the expectation of $\\Gamma(\\mathbf{p}, \\mathbf{q})$ over all possible $\\mathbf{p}$ or $\\mathbf{q}$:\n",
    "\\begin{align*}\n",
    "\\Gamma(\\mathbf{Z}_1, \\mathbf{Z}_2) =& \\Gamma(\\mathrm{E}[\\mathbf{p}],\\mathrm{E}[\\mathbf{q}])\\\\\n",
    "=&\\{x \\in \\mathbb{R}^2\\,\\vert\\,\\phi(x,[1,0]^T)=\\phi(x,[0,1]^T)\\}\\\\\n",
    "=&\\{x \\in \\mathbb{R}^2\\,\\vert\\, x_1 = x_2\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.3\n",
    "\n",
    "This excercise is related to ordered statistics, where the simplest case minimum is encountered. The volume of a $p$ dimmensional ball is proportional to $r^p$, where $r$ is its radius. Let's denote the random variable of the distance of any sampled point in this $p$ dimmentional unit ball to the origin as $Z$. Then,\n",
    "\\begin{align*}\n",
    "P(Z < x)=&x^p, \\\\\n",
    "P(Z > x)=&1-x^p, \\\\\n",
    "P(Z_{(1)} > x) =& P(\\prod_{i=1}^N Z_i > x)=\\left(1-x^p\\right)^N,\\\\\n",
    "P(Z_{(1)} < x) =& 1 - \\left(1-x^p\\right)^N.\n",
    "\\end{align*}\n",
    "The median is the $\\frac12$ quantile of $Z_{(1)}$, solve for $1-\\left(1-x^p\\right)^N=\\frac12$ gives us:\n",
    "$$x=\\left(1-\\frac12^{1/N}\\right)^{1/p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.4\n",
    "\n",
    "$z_i$ is a linear combination of all the elements of any realization of $\\mathbf{X}$, so it must be one dimensional normal. $\\mathbf{X} \\sim N(0, \\mathbf{I})$, therefore we have:\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[z_i]=& 0,\\\\\n",
    "\\mathrm{Var}(z_i)=&\\sum_{i=1}^p\\left(a_i^2\\cdot1\\right)=1.\n",
    "\\end{align*}\n",
    "\n",
    "The expected squared distance of $z_i$ is:\n",
    "$$\\mathrm{E}[\\mathrm{d}(z_i,0)^2]=\\mathrm{E}[z_i^2]=1.$$\n",
    "\n",
    "The expected squared distance for $x_0$ is:\n",
    "$$\\mathrm{E}[\\mathrm{d}(x_0,0)^2]=\\mathrm{E}[x_0^2]=p,$$\n",
    "since $x_0^2 \\sim \\chi^2(p).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.5\n",
    "\n",
    "## (a)\n",
    "\n",
    "Following the notion in (2.4), where $\\mathbf{X}$ is an $N\\times p$ matrix with each row an input vector of the training set, and $\\mathbf{y}$ is a $N$-vector of the output of the training set. The least square estimate of $\\beta$ is given by:\n",
    "$$\\hat{\\beta}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}.$$\n",
    "Then we have:\n",
    "\\begin{align*}\n",
    "    y_0 - \\hat{y}_0 =& x_0^T\\beta+\\varepsilon_0-x_0^T\\hat{\\beta} \\\\\n",
    "    =& x_0^T\\beta+\\varepsilon_0\n",
    "    -x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "    =& x_0^T\\beta+\\varepsilon_0 - x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\n",
    "    \\mathbf{X}^T\\left(\\mathbf{X}\\beta+\\varepsilon\\right) \\\\\n",
    "    =& \\varepsilon_0 - x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\varepsilon.\n",
    "\\end{align*}\n",
    "Knowing that $\\varepsilon_0$ and $\\varepsilon$ are independent:\n",
    "\\begin{align*}\n",
    "    \\mathrm{EPE}(x_0)=&\\mathrm{E}[(y_0 - \\hat{y}_0)^2]\\\\\n",
    "    =&\\mathrm{E}\\left[\\left(\\varepsilon_0 - x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\n",
    "    \\mathbf{X}^T\\varepsilon\\right)^2\\right] \\\\\n",
    "    =&\\mathrm{E}\\left[\\varepsilon_0^2\\right] + \\mathrm{E}\\left[\n",
    "    x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\varepsilon \n",
    "    \\varepsilon^T\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right]+0 \\\\\n",
    "    =&\\sigma^2+\\sigma^2\\mathrm{E}\\left[\n",
    "    x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\cdot\\mathbf{I}\\cdot\n",
    "    \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right] \\\\\n",
    "    =&\\sigma^2+\\sigma^2\\mathrm{E}\\left[x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right].\n",
    "\\end{align*}\n",
    "## (b)\n",
    "\n",
    "Assuming $\\mathrm{E}[\\mathbf{X}]=0$, by law of large number we have \n",
    "$$\\frac{\\mathbf{X}^T\\mathbf{X}}{N}\\rightarrow \\mathrm{Cov}(\\mathbf{x})\\, \n",
    "\\mathrm{when}\\, N\\rightarrow\\infty.$$\n",
    "Further more, \n",
    "$$\\frac{\\mathrm{Cov}(\\mathbf{x})^{-1}\\mathbf{X}^T\\mathbf{X}}{N}\\rightarrow \\mathbf{I}\\, \n",
    "\\mathrm{when}\\, N\\rightarrow\\infty.$$\n",
    "$x_0$ has the same distribution as any row of $\\mathbf{X}$:\n",
    "$$\\mathrm{E}[x_0x_0^T]=\\mathrm{Cov}(x_0)=\\mathrm{Cov}(\\mathbf{x}).$$\n",
    "Knowing that $x_0$ and $\\mathbf{X}$ are independent, and that both $\\mathrm{Cov}(\\mathbf{x})$ and $\\mathbf{X}^T\\mathbf{X}$ are symmetric and invertible (for the latter, almost surely):\n",
    "\\begin{align*}\n",
    "\\mathrm{E}\\left[x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right] =& \n",
    "\\mathrm{E}\\left[\\mathrm{Tr}\\left(x_0x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right)\\right]\\\\\n",
    "=&\\mathrm{Tr}\\left(\\mathrm{E}\\left[x_0x_0^T\\right]\\cdot\n",
    "\\mathrm{E}\\left[\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right)\\\\\n",
    "=&\\mathrm{Tr}\\left(\\mathrm{Cov}(\\mathbf{x})\\cdot\n",
    "\\mathrm{E}\\left[\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right)\\\\\n",
    "=&\\mathrm{Tr}\\left(\\mathrm{E}\\left[\\mathrm{Cov}(\\mathbf{x})\n",
    "\\cdot\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right)\\\\\n",
    "=& \\mathrm{Tr}\\left(\\mathrm{E}\\left[\\left(\\mathrm{Cov}(\\mathbf{x})^{-1}\n",
    "\\cdot\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right).\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The matrix inverse operator is a continuous operator on the space of $p\\times p$ invertible matrices, as a result, we have:\n",
    "$$\\left(\\mathrm{Cov}(\\mathbf{x})^{-1}\n",
    "\\cdot\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\sim \\left(N\\cdot\\mathbf{I}\\right)^{-1}\\,\n",
    "\\mathrm{when}\\, N\\rightarrow\\infty.$$\n",
    "One may notice that the matrix inverse operator is continuous but not uniformly continuous, so the above limit behavior holds true almost surely, since the uninvertible $p\\times p$ matrices consists a negligible subset of the set of all $p\\times p$ matrices. \n",
    "\n",
    "Finally we have:\n",
    "\\begin{align*}\n",
    "\\mathrm{E}\\left[x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right] \\sim&\n",
    "\\mathrm{Tr}\\left(\\mathrm{E}\\left[\\left(N\\cdot\\mathbf{I}\\right)^{-1}\\right]\\right)\\\\\n",
    "=& N^{-1}\\mathrm{Tr}(\\mathbf{I})\\\\\n",
    "=& p/N.\n",
    "\\end{align*}\n",
    "This completes the derivation of (2.28). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
