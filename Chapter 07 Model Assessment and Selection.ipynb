{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.4\n",
    "\n",
    "By definition, the expectation of optimism over training sets is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}-\\overline{\\mbox{err}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(Y_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}-\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}\\right]\\right)\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(\\mathbf{Y}_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}-\\mathbb{E}_{\\mathbf{y}}\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that $\\mathbf{Y}^0$ is another realization of $f(x)+\\varepsilon$, so it is independent of $\\mathbf{y}$, and thus $\\hat{f}(x)$, which is a linear function of $\\mathbf{y}$.\n",
    "\n",
    "As a result, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left(\\mathbf{Y}_{i}^{0}-\\hat{f}\\left(x_{i}\\right)\\right)^{2} & =\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}-2\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)+\\hat{f}\\left(x_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}\\right]-2\\mathbb{E}_{\\mathbf{y}}\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{f}\\left(x_{i}\\right)^{2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "At the same time:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left(y_{i}-\\hat{f}\\left(x_{i}\\right)\\right)^{2} & =\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}-2y_{i}\\hat{f}\\left(x_{i}\\right)+\\hat{f}\\left(x_{i}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}\\right]-2\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{f}\\left(x_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{f}\\left(x_{i}\\right)^{2}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "We also know that $y_i$ and $\\mathbf{Y}^0_i$ are iid, so $$\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\left(\\mathbf{Y}_{i}^{0}\\right)^{2}\\right]=\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}^{2}\\right].$$\n",
    "\n",
    "With all the above relations, and that $\\hat{y}_i=\\hat{f}(x_i)$, we can conclude that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}-\\overline{\\mbox{err}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{f}\\left(x_{i}\\right)-\\mathbb{E}_{\\mathbf{Y}^{o}}\\left[\\mathbf{Y}_{i}^{0}\\hat{f}\\left(x_{i}\\right)\\right]\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{y_{i}}-\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\right]\\hat{f}\\left(x_{i}\\right)\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\left(\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\hat{y_{i}}\\right]-\\mathbb{E}_{\\mathbf{y}}\\left[y_{i}\\right]\\mathbb{E}_{\\mathbf{y}}\\left[\\hat{y}_{i}\\right]\\right)\\\\\n",
    " & =\\frac{2}{N}\\sum_{i=1}^{N}\\mbox{Cov}\\left(y_{i},\\hat{y}_{i}\\right).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.5\n",
    "Let's assume that the true model is: $\\mathbf{y}=f(\\mathbf{x)}+\\mathbf{\\varepsilon}$, \n",
    "where $\\varepsilon$ is a $N$ vector of iid random variables with mean $0$ and variance $\\sigma_{\\varepsilon}$.\n",
    "\n",
    "With some matrix algebra we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{y}_{i},y_{i}\\right) & =\\sum_{i=1}^{N}\\mbox{Cov}\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)_{ii}\\\\\n",
    " & =\\mbox{Tr}\\left[\\mbox{Cov}\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left[\\mbox{Cov}\\left(\\mathbf{Sy},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left[\\mathbf{S}\\mbox{Cov}\\left(\\mathbf{y},\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\mbox{Tr}\\left(\\mathbf{S}\\sigma_{\\varepsilon}^{2}\\mathbf{I}\\right)\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\mathbf{I}\\right)\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}\\mbox{Tr}\\left(\\mathbf{S}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "It is worth mentioning that, in simple linear regression,\n",
    "$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}$, and in ridge regression with parameter $\\lambda$, \n",
    "$\\mathbf{S}=\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}.$\n",
    "\n",
    "We can easily prove that the trace for the former is $p$ and for the latter it is:\n",
    "$\\sum_{i=1}^{p}\\frac{d_{i}^{2}}{d_{i}^{2}+\\lambda}$, where $d_i$ is the $i$th singular value of the $N\\times p$ matrix $\\mathbf{X}$.\n",
    "\n",
    "There is a little logical gap we would like to fill here. Specifically, we would like to prove:\n",
    "$$\\mbox{Cov}\\left(\\mathbf{Sx},\\mathbf{y}\\right)=\\mathbf{S}\\mbox{Cov}\\left(\\mathbf{x},\\mathbf{y}\\right),$$\n",
    "where $\\mathbf{S}$ is a $m \\times n$ deterministic matrix, and $\\mathbf{x}$ is a random $n$-vector and $\\mathbf{y}$ is a random $m$-vector.\n",
    "\n",
    "The proof is a straight forward plug-in of definitions:\n",
    "\n",
    "\\begin{alignat*}{1}\n",
    "\\mbox{Cov}\\left(\\mathbf{Sx},\\mathbf{y}\\right)_{i,j} & =\\mbox{Cov}\\left(\\left(\\mathbf{Sx}\\right)_{i},\\mathbf{y}_{j}\\right)\\\\\n",
    " & =\\mbox{Cov}\\left(\\sum_{k=1}^{n}\\mathbf{S}_{ik}\\mathbf{x}_{k},\\mathbf{y}_{j}\\right)\\\\\n",
    " & =\\sum_{k=1}^{n}\\mathbf{S}_{ik}\\mbox{Cov}\\left(\\mathbf{x}_{k},\\mathbf{y}_{j}\\right)\\\\\n",
    " & =\\sum_{k=1}^{n}\\mathbf{S}_{ik}\\mbox{Cov}\\left(\\mathbf{x},\\mathbf{y}\\right)_{k,j}\\\\\n",
    " & =\\left(\\mathbf{S}\\mbox{Cov}\\left(\\mathbf{x},\\mathbf{y}\\right)\\right)_{i,j}.\n",
    "\\end{alignat*}\n",
    "\n",
    "We successfully proved that, in ordinary linear regression model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]+\\frac{2p\\sigma_{\\varepsilon}^{2}}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "In Ex. 2.9 we also explicitly derived the training error:\n",
    "$$\\mathbb{E}_{\\mathbf{y}}\\left[\\overline{\\mbox{err}}\\right]=\\sigma_{\\varepsilon}^{2}-\\frac{p}{N}\\sigma_{\\varepsilon}^{2}.$$\n",
    "\n",
    "So obviously the in-sample test error should be:\n",
    "$$\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]=\\sigma_{\\varepsilon}^{2}+\\frac{p}{N}\\sigma_{\\varepsilon}^{2}.$$\n",
    "\n",
    "We can also derive the formula from the definition and should obtain the same result.\n",
    "Suppose $\\mathbf{y}$ is a new realization of the $i$th training response, with corresponding predictor $\\mathbf{x}_i$, then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right] & =\\mathbb{E}_{\\mathbf{y}}\\left[\\left(\\mathbf{y}-\\hat{\\mathbf{y}}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{y}}\\left[\\left(\\varepsilon_{0}-\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}_{\\mathbf{y}}\\left[\\varepsilon_{0}^{2}\\right]+\\mathbb{E}_{\\mathbf{y}}\\left[\\left(\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\right)^{2}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\mathbf{\\varepsilon}^{T}\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{x}_{i}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbf{\\varepsilon}\\mathbf{\\varepsilon}^{T}\\right]\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{x}_{i}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n\\times n}\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{x}_{i}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\sigma_{\\varepsilon}^{2}\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{x}_{i}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\sigma_{\\varepsilon}^{2}\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}_{\\mathbf{y}}\\left[\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{x}_{i}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{N}\\mathbb{E}_{\\mathbf{y}}\\left[\\sum_{i=1}^{N}\\mathbf{x}_{i}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{x}_{i}\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{N}\\mathbb{E}_{\\mathbf{y}}\\left[\\mathrm{Tr}\\left(\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\right)\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{N}\\mathbb{E}_{\\mathbf{y}}\\left[\\mathrm{Tr}\\left(\\mathbf{X}^{T}\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\right)\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{N}\\mathbb{E}_{\\mathbf{y}}\\left[\\mathrm{Tr}\\left(\\mathbf{I}_{p\\times p}\\right)\\right]\\\\\n",
    " & =\\sigma_{\\varepsilon}^{2}+\\frac{p}{N}\\sigma_{\\varepsilon}^{2}\n",
    "\\end{align*}\n",
    "\n",
    "In Ex. 2.5 we also proved that the conditional (out-of-sample) test error $\\mathrm{Err}_{\\mathcal{T}}$ has the following asymptotic behavior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{Err}_{\\mathcal{T}} & \\sim\\sigma_{\\varepsilon}^{2}+\\frac{p}{N}\\sigma_{\\varepsilon}^{2},\\quad N\\rightarrow\\infty.\n",
    "\\end{align*}\n",
    "\n",
    "This shows that in OLS model, when the sample size is large enough, \n",
    "the three equivalent value: the Mallow's $C_p$ statistic, AIC, and the expected in-sample test error\n",
    " $\\mathbb{E}_{\\mathbf{y}}\\left[\\mbox{Err}_{\\mbox{in}}\\right]$ all generate a good estimate of the out-of-sample test error, conditional on the training set $\\mathrm{Err}_{\\mathcal{T}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.7\n",
    "Using the approximation $1/(1-x)^2 \\approx 1+2x$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{GCV} & =\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\frac{y_{i}-\\hat{f}\\left(x_{i}\\right)}{1-\\mbox{Tr}\\left(\\mathbf{S}\\right)/N}\\right]^{2}\\\\\n",
    " & \\approx\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2}\\left(1+\\frac{2\\mbox{Tr}\\left(\\mathbf{S}\\right)}{N}\\right)\\\\\n",
    " & =\\overline{\\mbox{err}}+2\\frac{\\mbox{Tr}\\left(\\mathbf{S}\\right)}{N}\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2}.\n",
    "\\end{align*}\n",
    "\n",
    "The trace of $\\mathbf{S}$ is the effective degree of free down of the linear smoother $\\mathbf{S}$. If we use average square error of estimator $\\hat{f}$ as estimator of the true error:\n",
    "\n",
    "$$\\hat{\\sigma}_{\\varepsilon}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_{i}-\\hat{f}\\left(x_{i}\\right)\\right]^{2},$$ then we know that GCV is an approximation of $C_p$ when $\\mbox{Tr}\\left(\\mathbf{S}\\right)/N$ is small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.8\n",
    "\n",
    "The idea is to find a way to construct $\\alpha$ given the classes of $z^1,\\dots,z^l$, \n",
    "such that $\\alpha z^i$ is in $(0, \\pi)$ if $z^i$ is of class 0 and in $(\\pi, 2\\pi)$ is \n",
    "$z^i$ is of class 1.\n",
    "\n",
    "We want to achieve the goal that the value of $\\sin\\left(\\alpha z^{i}\\right)$ is dominantly determined by $y^i$. \n",
    "Let's denote the class of $z^i$ by $y_i$, then $\\alpha$ must be a function of $y_1,\\dots, y_l$.\n",
    "Let's consider the case that $\\alpha$ is a linear combination of $y_i$:\n",
    "\n",
    "$$\\alpha = \\sum_i^la_iy_i.$$\n",
    "\n",
    "We want the value of $\\sin\\left(\\alpha z^{i}\\right)$ to be insensitive to the values of $y^j$, where $j \\ne i$.\n",
    "The are only two ways to for the sine function to be insensitive to the change of its independent variable: either the change is too small compared with $\\pi$, or the change is a multiple of $2\\pi$.\n",
    "Let's further look at the decomposition of the independent variable:\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha z^{j} & =\\sum_{i=1}^{l}a_{i}y^{i}z^{j}\\\\\n",
    " & =\\sum_{i=1}^{j-1}a_{i}y^{i}z^{j}+a_{j}y^{j}z^{j}+\\sum_{i=j+1}^{l}a_{i}y^{i}z^{j}\n",
    "\\end{align*}\n",
    "\n",
    "In order to achieve the goal, we want the value of $a_{j}y^{j}z^{j}$ to be close to $\\pi y^j$, the value of $\\sum_{i=1}^{j-1}a_{i}y^{i}z^{j}$ to be close to 0, and the value of $\\sum_{i=j+1}^{l}a_{i}y^{i}z^{j}$ to be a multiple of $2\\pi$.\n",
    "\n",
    "An obvious choice is $a_i=10^i\\pi$, in that case:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{j-1}a_{i}y^{i}z^{j} & =\\pi z^{j}\\sum_{i=1}^{j-1}10^{i}y^{i}\\\\\n",
    " & \\in\\left[0,\\pi\\sum_{i=1}^{j-1}10^{i-j}\\right)\\\\\n",
    " & =\\left[0,\\pi\\frac{1-10^{1-j}}{9}\\right)\\\\\n",
    " & \\in\\left[0,\\pi\\right)\n",
    "\\end{align*}\n",
    "\n",
    "$$a_{j}y^{j}z^{j}=\\pi y^{j}=\\begin{cases}\n",
    "0 & y^{j}=0\\\\\n",
    "\\pi & y^{j}=1\n",
    "\\end{cases}$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=j+1}^{l}a_{i}y^{i}z^{j} & =\\pi\\sum_{i=j+1}^{l}10^{i-j}y^{i}\\\\\n",
    " & =\\pi\\sum_{i=1}^{l-j}10^{i}y^{i}\\\\\n",
    " & =2\\pi\\cdot5\\sum_{i=0}^{l-j-1}10^{i}y^{i}\n",
    "\\end{align*}\n",
    "\n",
    "As a result, we can separate the points $z^j$ with label $y^j$ in this way:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sin(az^{j})= & \\sin\\left(\\pi\\sum_{i=1}^{j-1}10^{i-j}y^{i}+\\pi y^{j}\\right)\\\\\n",
    "= & \\begin{cases}\n",
    "\\sin\\left[0,\\pi\\right) & y^{j}=0\\\\\n",
    "\\sin\\left[\\pi,2\\pi\\right) & y^{j}=1\n",
    "\\end{cases}\\\\\n",
    "\\in & \\begin{cases}\n",
    "\\left[0,1\\right) & y^{j}=0\\\\\n",
    "\\left(-1,0\\right] & y^{j}=1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "This is not satisfying enough, because sometimes the function value can be $0$ and thus lies in the class boundary.\n",
    "To solve this problem we introduce an additional term to $a$,\n",
    "$$a=\\pi\\left(1+\\sum_{i=1}^{l}10^{i}y^{i}\\right)$$\n",
    "\n",
    "This additional small term will perturb the function slightly away from 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(\\pi+\\sum_{i=1}^{j-1}a_{i}y^{i}\\right)z^{j} & =\\pi z^{j}\\left(1+\\sum_{i=1}^{j-1}10^{i}y^{i}\\right)\\\\\n",
    " & \\in\\left[\\pi10^{-j},\\pi\\sum_{i=0}^{j-1}10^{i-j}\\right)\\\\\n",
    " & =\\left[\\pi10^{-j},\\pi\\frac{1-10^{-j}}{9}\\right)\\\\\n",
    " & \\subset\\left(0,\\pi\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Finally we proved that there exists $a=\\pi\\left(1+\\sum_{i=1}^{l}10^{i}y^{i}\\right)$, \n",
    "such that the function $\\sin(ax)$ can shatter the points $\\{10^{-j}\\}_{j=1}^l$ \n",
    "with arbitrarily large $l$. That is to say, the VC dimension of the set of functions $\\{I(\\sin(ax))>0\\}$ is infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 7.9\n",
    "Here we use the R package \"leaps\" to find the best subset with given number predictors. \n",
    "In the book, the best predictors model has the lowest RSS, so we choose to use the $R^2$ method for leaps. Since minimizing RSS is equivalent to maximizing $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "import pandas as pd\n",
    "from os import getcwd\n",
    "fname = getcwd() + '/data/Chapter03/prostate.data'\n",
    "prostate = pd.read_table(fname, delim_whitespace=True, index_col=0)\n",
    "col_pred = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n",
    "col_resp = 'lpsa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i prostate,col_pred,col_resp -o model_subset\n",
    "library(\"leaps\")\n",
    "X = data.matrix(prostate[col_pred])\n",
    "Y = data.matrix(prostate[col_resp])\n",
    "model_subset = leaps(x=X, y=Y, nbest=1, names=colnames(X), method='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size\n",
      "[2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "which\n",
      "   lcavol  lweight  age  lbph  svi  lcp  gleason  pgg45\n",
      "1       1        0    0     0    0    0        0      0\n",
      "2       1        1    0     0    0    0        0      0\n",
      "3       1        1    0     0    1    0        0      0\n",
      "4       1        1    0     1    1    0        0      0\n",
      "5       1        1    1     1    1    0        0      0\n",
      "6       1        1    1     1    1    0        0      1\n",
      "7       1        1    1     1    1    1        0      1\n",
      "8       1        1    1     1    1    1        1      1\n",
      "label\n",
      "['(Intercept)' 'lcavol' 'lweight' 'age' 'lbph' 'svi' 'lcp' 'gleason'\n",
      " 'pgg45']\n",
      "r2\n",
      "[0.5394319707818949, 0.5955040420539792, 0.6359499009362206, 0.6435560770281128, 0.6526149542340808, 0.6577800565956129, 0.6630054182032703, 0.6633895654989244]\n"
     ]
    }
   ],
   "source": [
    "#import pandas.rpy.common as com\n",
    "import sys\n",
    "sys.path.append('script')# all the python scripts used by notebooks in the repo are in script\n",
    "from Converters import convert_robj\n",
    "model = convert_robj(model_subset)\n",
    "for k, v in model.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
